{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "### Step 1: resize/crop images\n",
    "Requires having run: `beescrape.py`\n",
    "Depends on yolov5 and `shutil`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "import os\n",
    "from os import listdir\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import skimage\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = '/home/wilber/Documents/RESEARCH/research/beespotter'\n",
    "OUT_DIR = '/m2docs/res/data'\n",
    "CROPPED_PATH = '/m2docs/res/cropped_imgs'\n",
    "cropped_files = listdir(CROPPED_PATH)\n",
    "\n",
    "# Args:\n",
    "VAL_SIZE = 256\n",
    "TEST_SIZE = 128\n",
    "preprocess = True\n",
    "resize = True\n",
    "length = 256\n",
    "size = (length, length) # (512, 512)\n",
    "\n",
    "# Uncropped args: (use the same images except without passing through yolo bee finder for comparison)\n",
    "OUT_DIR_RAW = '/m2docs/res/data_raw'\n",
    "UNCROPPED_PATH = '/m2docs/res/uncropped_imgs'\n",
    "COPY_UNCROPPED = True\n",
    "\n",
    "# Species with >= 1000 images:\n",
    "classes = ['Apis_mellifera','Bombus_impatiens','Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/m2docs/res/data_raw/*': No such file or directory\n",
      "resizing images to (256, 256)\n",
      "Apis_mellifera skipped 1540\n",
      "Apis_mellifera total 1217\n",
      "Bombus_impatiens skipped 1296\n",
      "Bombus_impatiens total 2017\n",
      "Bombus_auricomus skipped 469\n",
      "Bombus_auricomus total 724\n",
      "Bombus_bimaculatus skipped 629\n",
      "Bombus_bimaculatus total 1111\n",
      "Bombus_griseocollis skipped 1111\n",
      "Bombus_griseocollis total 1831\n",
      "Images moved and rotated. Skipped  5045\n"
     ]
    }
   ],
   "source": [
    "%rm -R /m2docs/res/data/*\n",
    "%rm -R /m2docs/res/data_raw/*\n",
    "\n",
    "print(\"resizing images to {}\".format(size))\n",
    "\n",
    "# from https://gist.github.com/Prasad9/28f6a2df8e8d463c6ddd040f4f6a028a\n",
    "noise_modes = [None,'salt','pepper','s&p']\n",
    "NOISE_AMOUNT = .01 # default amount\n",
    "def add_noise(img, mode, noise_amount = .03):\n",
    "    if mode is not None:\n",
    "        gimg = skimage.util.random_noise(img, mode = mode, amount = random.uniform(0,noise_amount))\n",
    "        return gimg\n",
    "    else:\n",
    "        print(\"oops, you shouldn't see this\")\n",
    "\n",
    "total_skipped = 0\n",
    "for label in classes:  # for each type of bee\n",
    "    skipnum = 0\n",
    "    classID = classes.index(label)\n",
    "    \n",
    "    input_path = os.path.join(DATA_DIR, label)\n",
    "    im_list = os.listdir(input_path)\n",
    "    random.shuffle(im_list)\n",
    "    \n",
    "    # Attempt to create directories:\n",
    "    if not os.path.exists(OUT_DIR + '/train/' + label):\n",
    "        os.makedirs(OUT_DIR + '/train/' + label)\n",
    "    if not os.path.exists(OUT_DIR + '/test/' + label):\n",
    "        os.makedirs(OUT_DIR + '/test/' + label)\n",
    "    if not os.path.exists(OUT_DIR + '/valid/' + label):\n",
    "        os.makedirs(OUT_DIR + '/valid/' + label)\n",
    "    if not os.path.exists(OUT_DIR_RAW + '/train/' + label):\n",
    "        os.makedirs(OUT_DIR_RAW + '/train/' + label)\n",
    "    if not os.path.exists(OUT_DIR_RAW + '/test/' + label):\n",
    "        os.makedirs(OUT_DIR_RAW + '/test/' + label)\n",
    "    if not os.path.exists(OUT_DIR_RAW + '/valid/' + label):\n",
    "        os.makedirs(OUT_DIR_RAW + '/valid/' + label)\n",
    "        \n",
    "    train_path = os.path.join(OUT_DIR,'train/'+ label + '/')\n",
    "    test_path  = os.path.join(OUT_DIR,'test/' + label + '/')\n",
    "    valid_path = os.path.join(OUT_DIR,'valid/' + label + '/')\n",
    "    train_path_raw = os.path.join(OUT_DIR_RAW,'train/'+ label + '/')\n",
    "    test_path_raw  = os.path.join(OUT_DIR_RAW,'test/' + label + '/')\n",
    "    valid_path_raw = os.path.join(OUT_DIR_RAW,'valid/' + label + '/')\n",
    "    \n",
    "    index = 0\n",
    "    for img in im_list:\n",
    "        if img in cropped_files:\n",
    "            pic = Image.open(os.path.join(CROPPED_PATH,img))\n",
    "            \n",
    "            if (COPY_UNCROPPED):\n",
    "                    pic2 = Image.open(os.path.join(UNCROPPED_PATH,img))\n",
    "            \n",
    "            if resize: #resize image\n",
    "                out = pic.resize(size)\n",
    "                if (COPY_UNCROPPED):\n",
    "                    out2 = pic2.resize(size)\n",
    "            else:\n",
    "                out = pic\n",
    "                if (COPY_UNCROPPED):\n",
    "                    out2 = pic2\n",
    "            \n",
    "            if index < VAL_SIZE:\n",
    "                out.save(os.path.join(valid_path,img))\n",
    "            elif index < VAL_SIZE + TEST_SIZE:\n",
    "                out.save(os.path.join(test_path,img))\n",
    "            if index < VAL_SIZE and COPY_UNCROPPED:\n",
    "                out2.save(os.path.join(valid_path_raw,img))\n",
    "            elif index < VAL_SIZE + TEST_SIZE and COPY_UNCROPPED:\n",
    "                out2.save(os.path.join(test_path_raw,img))    \n",
    "            else:    #training set, rotate\n",
    "                out.save(os.path.join(train_path,img.replace('.jpg','-0.jpg')))\n",
    "                if COPY_UNCROPPED:\n",
    "                    out2.save(os.path.join(train_path_raw,img.replace('.jpg','-0.jpg')))\n",
    "                if preprocess:\n",
    "                    for rot,mode,ext in zip([random.randint(0,359),random.randint(0,359),random.randint(0,359)],[random.choice(noise_modes),random.choice(noise_modes),random.choice(noise_modes)],[1,2,3]):\n",
    "                        #print(' ',img,rot,mode,ext,pic.mode)\n",
    "                        if mode is not None:\n",
    "                            Image.fromarray((add_noise(np.array(out.rotate(rot)),mode,NOISE_AMOUNT)*255).astype(np.uint8),pic.mode).save(os.path.join(train_path,img.replace('.jpg','-'+str(ext)+'.jpg')))\n",
    "                            if COPY_UNCROPPED:\n",
    "                                Image.fromarray((add_noise(np.array(out2.rotate(rot)),mode,NOISE_AMOUNT)*255).astype(np.uint8),pic2.mode).save(os.path.join(train_path_raw,img.replace('.jpg','-'+str(ext)+'.jpg')))\n",
    "                        else:\n",
    "                            out.rotate(rot).save(os.path.join(train_path,img.replace('.jpg','-'+str(ext)+'.jpg')))\n",
    "                            if COPY_UNCROPPED:\n",
    "                                out2.rotate(rot).save(os.path.join(train_path_raw,img.replace('.jpg','-'+str(ext)+'.jpg')))\n",
    "                \n",
    "        else:\n",
    "            skipnum += 1\n",
    "            index -= 1\n",
    "            pass\n",
    "        index += 1\n",
    "    print(label, \"skipped\", skipnum)\n",
    "    print(label, \"total\", index)\n",
    "    \n",
    "    total_skipped += skipnum\n",
    "    \n",
    "print(\"Images moved and rotated. Skipped \", total_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apis_mellifera\n",
      "images in /train/: 3332\n",
      "images in /train/: 3332 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "Bombus_impatiens\n",
      "images in /train/: 6532\n",
      "images in /train/: 6532 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "Bombus_auricomus\n",
      "images in /train/: 1360\n",
      "images in /train/: 1360 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "Bombus_bimaculatus\n",
      "images in /train/: 2908\n",
      "images in /train/: 2908 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "Bombus_griseocollis\n",
      "images in /train/: 5788\n",
      "images in /train/: 5788 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "[3332, 6532, 1360, 2908, 5788]\n",
      "[3332, 6532, 1360, 2908, 5788]\n"
     ]
    }
   ],
   "source": [
    "import fnmatch\n",
    "cropped_count = []\n",
    "uncropped_count = []\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    cropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR + '/train/' + c + \"/\")), '*.jpg')))\n",
    "    uncropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR_RAW + '/train/' + c + \"/\")), '*.jpg')))\n",
    "    for t in ['/train/','/test/','/valid/']:\n",
    "        print(\"images in {}: {}\".format(t,len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR + t + c + \"/\")), '*.jpg'))))\n",
    "        print(\"images in {}: {} (uncropped)\".format(t,len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR_RAW + t + c + \"/\")), '*.jpg'))))\n",
    "print(cropped_count)\n",
    "print(uncropped_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apis_mellifera\n",
      "Bombus_impatiens\n",
      "Bombus_auricomus\n",
      "Bombus_bimaculatus\n",
      "Bombus_griseocollis\n",
      "['Apis_mellifera', 'Bombus_impatiens', 'Bombus_auricomus', 'Bombus_bimaculatus', 'Bombus_griseocollis']\n",
      "[3332, 6532, 1360, 2908, 5788]\n",
      "[3332, 6532, 1360, 2908, 5788]\n",
      "Target number of images: 7185.200000000001\n",
      "Copying cropped images...\n",
      "[7186, 7186, 7186, 7186, 7186]\n",
      "Done.\n",
      "\n",
      "Copying uncropped images...\n",
      "['Apis_mellifera', 'Bombus_impatiens', 'Bombus_auricomus', 'Bombus_bimaculatus', 'Bombus_griseocollis']\n",
      "[3332, 6532, 1360, 2908, 5788]\n",
      "Target count: n = 7185.200000000001\n",
      "[7186, 7186, 7186, 7186, 7186]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Flip images over axes to even out training sets:\n",
    "import fnmatch\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import skimage\n",
    "\n",
    "classes = ['Apis_mellifera','Bombus_impatiens','Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n",
    "ABS_PATH_TRAIN = '/m2docs/res/data/train'\n",
    "MULTIPLIER = 1.2 # how many times more images need to be created: mult * len(largest class)\n",
    "verbose = False\n",
    "\n",
    "cropped_count = []\n",
    "uncropped_count = []\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    cropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR + '/train/' + c + \"/\")), '*.jpg')))\n",
    "    uncropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR_RAW + '/train/' + c + \"/\")), '*.jpg')))\n",
    "\n",
    "print(classes)\n",
    "print(cropped_count)\n",
    "print(uncropped_count)\n",
    "\n",
    "# add more images to largest class,\n",
    "# and bring the others to the same count\n",
    "target = max(cropped_count) * 1.1\n",
    "print(\"Target number of images: {}\".format(target))\n",
    "    \n",
    "print(\"Copying cropped images...\")\n",
    "\n",
    "noise_modes = [None,'salt','pepper','s&p']\n",
    "NOISE_AMOUNT = .01 # default amount\n",
    "def add_noise(img, mode, noise_amount = .03):\n",
    "    if mode is not None:\n",
    "        gimg = skimage.util.random_noise(img, mode = mode, amount = random.uniform(0, noise_amount))\n",
    "        return gimg\n",
    "    else:\n",
    "        print(\"oops, you shouldn't see this\")\n",
    "\n",
    "for c_name in classes:\n",
    "    current = cropped_count[classes.index(c_name)]\n",
    "    if (verbose): \n",
    "        print(c_name, current, \"->\", target)\n",
    "    im_list = os.listdir(os.path.join(ABS_PATH_TRAIN, c_name))\n",
    "    while current < target:\n",
    "        filename = random.choice(im_list)\n",
    "        if(verbose):\n",
    "            print(filename)\n",
    "        out = Image.open(os.path.join(ABS_PATH_TRAIN, c_name, filename))\n",
    "        \n",
    "        rot = random.randint(0,359)\n",
    "        mode = random.choice(noise_modes)\n",
    "        if (verbose): \n",
    "            print(filename,rot,mode,out.mode)\n",
    "        if (verbose): \n",
    "            print(filename.replace('.jpg','-x'+str(current)+'.jpg'))\n",
    "        if mode is not None:\n",
    "            Image.fromarray((add_noise(np.array(out.rotate(rot)),mode,NOISE_AMOUNT)*255).astype(np.uint8),pic.mode).save(os.path.join(ABS_PATH_TRAIN, c_name, filename.replace('.jpg','-x'+str(current)+'.jpg')))\n",
    "        else:\n",
    "            out.rotate(rot).save(os.path.join(ABS_PATH_TRAIN, c_name, filename.replace('.jpg','-x'+str(current)+'.jpg')))\n",
    "        current += 1\n",
    "\n",
    "cropped_count = []\n",
    "for c in classes:\n",
    "    cropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR + '/train/' + c + \"/\")), '*.jpg')))\n",
    "print(cropped_count)\n",
    "print(\"Done.\\n\")\n",
    "\n",
    "if COPY_UNCROPPED:\n",
    "    print(\"Copying uncropped images...\")\n",
    "    ABS_PATH_TRAIN_RAW = '/m2docs/res/data_raw/train'\n",
    "    c_count = [len(fnmatch.filter(os.listdir(os.path.join(ABS_PATH_TRAIN_RAW,c_name)), '*')) for c_name in classes]\n",
    "    print(classes)\n",
    "    print(c_count)\n",
    "\n",
    "    # add more images to largest class,\n",
    "    # and bring the others to the same count\n",
    "    print(\"Target count: n =\",target)\n",
    "\n",
    "    noise_modes = [None,'salt','pepper','s&p']\n",
    "    NOISE_AMOUNT = .01 # default amount\n",
    "    def add_noise(img, mode, noise_amount = .03):\n",
    "        if mode is not None:\n",
    "            gimg = skimage.util.random_noise(img, mode = mode, amount = random.uniform(0, noise_amount))\n",
    "            return gimg\n",
    "        else:\n",
    "            print(\"oops, you shouldn't see this\")\n",
    "\n",
    "    for c_name in classes:\n",
    "        current = uncropped_count[classes.index(c_name)]\n",
    "        if (verbose): \n",
    "            print(c_name, current, \"->\", target)\n",
    "        im_list = os.listdir(os.path.join(ABS_PATH_TRAIN_RAW, c_name))\n",
    "        while current < target:\n",
    "            filename = random.choice(im_list)\n",
    "            if (verbose): \n",
    "                print(filename)\n",
    "            out = Image.open(os.path.join(ABS_PATH_TRAIN_RAW, c_name, filename))\n",
    "\n",
    "            rot = random.randint(0,359)\n",
    "            mode = random.choice(noise_modes)\n",
    "            if (verbose): \n",
    "                print(filename,rot,mode,out.mode)\n",
    "            if (verbose): \n",
    "                print(filename.replace('.jpg','-x'+str(current)+'.jpg'))\n",
    "            if mode is not None:\n",
    "                Image.fromarray((add_noise(np.array(out.rotate(rot)),mode,NOISE_AMOUNT)*255).astype(np.uint8),pic.mode).save(os.path.join(ABS_PATH_TRAIN_RAW, c_name, filename.replace('.jpg','-x'+str(current)+'.jpg')))\n",
    "            else:\n",
    "                out.rotate(rot).save(os.path.join(ABS_PATH_TRAIN_RAW, c_name, filename.replace('.jpg','-x'+str(current)+'.jpg')))\n",
    "            current += 1\n",
    "\n",
    "    uncropped_count = []\n",
    "    for c in classes:\n",
    "        uncropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR_RAW + '/train/' + c + \"/\")), '*.jpg')))\n",
    "    print(uncropped_count)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os, datetime, math\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp \n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier\n",
    "Based on [this](https://nextjournal.com/gkoehler/pytorch-mnist) pytorch tutorial\n",
    "\n",
    "Load images and train classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length = 256\n",
    "skip_all = False\n",
    "\n",
    "classes = ['Apis_mellifera','Bombus_impatiens','Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n",
    "#classes = ['Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n",
    "\n",
    "ABS_PATH_TRAIN = '/m2docs/res/data/train'\n",
    "ABS_PATH_VALID = '/m2docs/res/data/valid'\n",
    "ABS_PATH_TEST = '/m2docs/res/data/test'\n",
    "ABS_PATH_TRAIN_RAW = '/m2docs/res/data_raw/train'\n",
    "ABS_PATH_VALID_RAW = '/m2docs/res/data_raw/valid'\n",
    "ABS_PATH_TEST_RAW = '/m2docs/res/data_raw/test'\n",
    "\n",
    "class SaveFeatures():\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "class FilterVisualizer():\n",
    "        def __init__(self, network, OUTPUT_DIR):\n",
    "                self.model = nn.Sequential(*list(network.children())[:-2]).cuda().eval()\n",
    "                self.network = network\n",
    "                self.OUTPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "        def visualize(self, sz, layer, filter, upscaling_steps=12, upscaling_factor=1.2, lr=0.1, opt_steps=20, blur=None, save=False, print_losses=False):\n",
    "                with Torch.no_grad():\n",
    "                    img = (np.random.random((sz, sz, 3)) * 20 + 128.)/255.\n",
    "    #                img = np.random.uniform(0, 1, size=(sz, sz, 3)).astype(np.float32)\n",
    "    #                median_filter_size = 4 if sz < 100 else 8\n",
    "    #                img = scipy.ndimage.filters.median_filter(img, [median_filter_size,median_filter_size,1])\n",
    "\n",
    "                    activations = SaveFeatures(layer)  # register hook\n",
    "\n",
    "                    for i in range(upscaling_steps):  # scale the image up upscaling_steps times\n",
    "                            train_tfms, val_tfms = tfms_from_model(network, sz)\n",
    "                            img_var = V(val_tfms(img)[None], requires_grad=True)  # convert image to Variable that requires grad\n",
    "                            optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n",
    "                            if i > upscaling_steps/2:\n",
    "                                    opt_steps_ = int(opt_steps*1.3)\n",
    "                            else:\n",
    "                                    opt_steps_ = opt_steps\n",
    "                            for n in range(opt_steps_):  # optimize pixel values for opt_steps times\n",
    "                                    optimizer.zero_grad()\n",
    "                                    self.model(img_var)\n",
    "                                    loss = -1 * activations.features[0, filter].mean()\n",
    "                                    if print_losses:\n",
    "                                            if i%3==0 and n%5==0:\n",
    "                                                    print(f'{i} - {n} - {float(loss)}')\n",
    "                                    loss.backward()\n",
    "                                    optimizer.step()\n",
    "                            img = val_tfms.denorm(np.rollaxis(to_np(img_var.data),1,4))[0]\n",
    "                            self.output = img\n",
    "                            sz = int(upscaling_factor * sz)  # calculate new image size\n",
    "                            img = cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC)  # scale image up\n",
    "                            if blur is not None: img = cv2.blur(img,(blur,blur))  # blur image to reduce high frequency patterns\n",
    "                    activations.close()\n",
    "                    return np.clip(self.output, 0, 1)\n",
    "        \n",
    "        def get_transformed_img(self,img,sz):\n",
    "            with Torch.no_grad():\n",
    "                train_tfms, val_tfms = tfms_from_model(network, sz)\n",
    "                return val_tfms.denorm(np.rollaxis(to_np(val_tfms(img)[None]),1,4))[0]\n",
    "        \n",
    "        def most_activated(self, image, layer, limit_top=None):\n",
    "            with Torch.no_grad():\n",
    "                train_tfms, val_tfms = tfms_from_model(network, 224)\n",
    "                transformed = val_tfms(image)\n",
    "\n",
    "                activations = SaveFeatures(layer)  # register hook\n",
    "                self.model(V(transformed)[None]);\n",
    "                \n",
    "                mean_act = [activations.features[0,i].mean().data.cpu().numpy()[0] for i in range(activations.features.shape[1])]\n",
    "                activations.close()\n",
    "                return mean_act\n",
    "\n",
    "def plot_reconstructions_single_layer(imgs,layer_name,filters,\n",
    "                                      n_cols=3,\n",
    "                                      cell_size=4,save_fig=True,\n",
    "                                      album_hash=None):\n",
    "        n_rows = ceil((len(imgs))/n_cols)\n",
    "\n",
    "        fig,axes = plt.subplots(n_rows,n_cols, figsize=(cell_size*n_cols,cell_size*n_rows))\n",
    "                    \n",
    "        for i,ax in enumerate(axes.flat):\n",
    "                ax.grid(False)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "\n",
    "                if i>=len(filters):\n",
    "                        pass\n",
    "\n",
    "                ax.set_title(f'fmap {filters[i]}')\n",
    "\n",
    "                ax.imshow(imgs[i])\n",
    "        fig.suptitle(f'cnn {layer_name}', fontsize=\"x-large\",y=1.0)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.88)\n",
    "        save_name = layer_name.lower().replace(' ','_')\n",
    "        if save_fig:\n",
    "                plt.savefig(f'' + OUTPUT_DIR + 'network_{save_name}_fmaps_{\"_\".join([str(f) for f in filters])}.png')\n",
    "                plt.close()\n",
    "                return True\n",
    "        else:\n",
    "                plt.show()\n",
    "                return None\n",
    "\n",
    "def reconstructions_single_layer(layer,layer_name,filters,\n",
    "                                 init_size=56, upscaling_steps=12, \n",
    "                                 upscaling_factor=1.2, \n",
    "                                 opt_steps=20, blur=5,\n",
    "                                 lr=1e-1,print_losses=False,\n",
    "                                 n_cols=3, cell_size=4,\n",
    "                                 save_fig=True,album_hash=None):\n",
    "        \n",
    "        imgs = []\n",
    "        for i in range(len(filters)):\n",
    "                imgs.append(FV.visualize(init_size,layer, filters[i], \n",
    "                            upscaling_steps=upscaling_steps, \n",
    "                            upscaling_factor=upscaling_factor, \n",
    "                            opt_steps=opt_steps, blur=blur,\n",
    "                            lr=lr,print_losses=print_losses))\n",
    "                \n",
    "        return plot_reconstructions_single_layer(imgs,layer_name,filters,\n",
    "                                                 n_cols=n_cols,cell_size=cell_size,\n",
    "                                                 save_fig=save_fig,album_hash=album_hash)\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def load(dir_name, batch_size, shuffle = False):\n",
    "    return(\n",
    "        #create a data loader\n",
    "        torch.utils.data.DataLoader(\n",
    "            datasets.ImageFolder(root = dir_name, \n",
    "                                 transform = transforms.Compose([\n",
    "                                     transforms.Resize((length,length)),\n",
    "                                     transforms.ToTensor()\n",
    "                                 ])),\n",
    "            batch_size = batch_size,\n",
    "            num_workers = 8,\n",
    "            shuffle = shuffle,\n",
    "        )\n",
    "    )\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout2d(p = hparams[HP_DROPOUT])\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, int(hparams[HP_NUM_UNITS]/4), 5)\n",
    "        #self.conv2 = nn.Conv2d(int(hparams[HP_NUM_UNITS]/16), int(hparams[HP_NUM_UNITS]/12), 2)\n",
    "        #self.conv3 = nn.Conv2d(int(hparams[HP_NUM_UNITS]/12), int(hparams[HP_NUM_UNITS]/8), 2)\n",
    "        #self.conv4 = nn.Conv2d(int(hparams[HP_NUM_UNITS]/8), int(hparams[HP_NUM_UNITS]/4), 5)\n",
    "        self.conv5 = nn.Conv2d(int(hparams[HP_NUM_UNITS]/4), int(hparams[HP_NUM_UNITS]/2), 5)\n",
    "        self.conv6 = nn.Conv2d(int(hparams[HP_NUM_UNITS]/2), hparams[HP_NUM_UNITS], 5)\n",
    "                \n",
    "        self.fc1 = nn.Linear(hparams[HP_NUM_UNITS]*60**2, length)\n",
    "        self.fc2 = nn.Linear(length, int(length/2))\n",
    "        self.fc3 = nn.Linear(int(length/2), int(length/4))\n",
    "        self.fc4 = nn.Linear(int(length/4), 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "#         x = self.dropout(F.relu(self.pool(self.conv2(x))))\n",
    "#         x = self.dropout(F.relu(self.pool(self.conv3(x))))\n",
    "#         x = self.dropout(F.relu(self.pool(self.conv4(x))))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv5(x))))\n",
    "        x = (F.relu(self.pool(self.conv6(x))))\n",
    "        print(x.size())\n",
    "        x = x.view(x.size(0),-1)\n",
    "        print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "def validate(network,device,load_valid,optimizer,criterion = nn.CrossEntropyLoss()):\n",
    "    network.train().to(device)\n",
    "    correct = 0\n",
    "    valid_loss = 0\n",
    "    for index, data in enumerate(load_valid, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        # gather accuracy stats:\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).float().sum().item()\n",
    "    print(\" Validation correct: {} / {}\".format(correct,len(load_valid.dataset)))  \n",
    "    accuracy = 100 * correct / len(load_valid.dataset)\n",
    "    valid_loss = valid_loss / len(load_valid.dataset)\n",
    "    return accuracy, valid_loss\n",
    "\n",
    "def test(network,device,load_test,criterion = nn.CrossEntropyLoss()):\n",
    "    network.eval().to(device)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(load_test, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = network(inputs).to(device)\n",
    "            test_loss += nn.functional.nll_loss(outputs, labels).item()\n",
    "\n",
    "            # gather accuracy stats:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "    print(\" Testing correct: {} / {}\".format(correct,len(load_test.dataset)))        \n",
    "    accuracy = 100 * correct / len(load_test.dataset)\n",
    "    valid_loss = test_loss / len(load_test.dataset)\n",
    "    return (accuracy, test_loss)\n",
    "\n",
    "def train(max_epochs, min_epochs, epoch_stretch, batch_size, train_path, valid_path, test_path, labels, hparams, writer):\n",
    "    epochs = max_epochs\n",
    "    class_names = labels\n",
    "    num_classes = len(class_names)\n",
    "    train_batch = batch_size\n",
    "    test_batch = 128\n",
    "    SAVE_PATH = '/m2docs/res/trained_models/model'\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device: {}\".format(device))\n",
    "    net = Net(hparams).to(device)\n",
    "    print(net)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    tag = datetime.datetime.now().strftime(\".%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    VISUAL_OUT = '/m2docs/res/visualizations/' + tag + \"/\"\n",
    "    if not os.path.exists(VISUAL_OUT):\n",
    "        os.makedirs(VISUAL_OUT)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Choose optimizer: (from ['adam','sgd','adagrad'])\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    if (hparams[HP_OPTIMIZER] == 'adam'):\n",
    "        optimizer = optim.Adam(net.parameters(), lr = .001)\n",
    "    elif (hparams[HP_OPTIMIZER] == 'sgd'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr = .001, momentum=0.1)\n",
    "    elif (hparams[HP_OPTIMIZER] == 'adagrad'):\n",
    "        optimizer = optim.Adagrad(net.parameters(), lr = .001)\n",
    "    else:\n",
    "        # default\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "    torch.manual_seed(417)\n",
    "    \n",
    "    load_train = load(train_path, batch_size, shuffle=True)\n",
    "    load_valid = load(valid_path, batch_size, shuffle=True)\n",
    "    load_test  = load(test_path, batch_size, shuffle=True)\n",
    "\n",
    "    validation_accuracies = []\n",
    "    best_epoch = 0\n",
    "    epoch = 0\n",
    "    killed = False\n",
    "    while (epoch <= best_epoch + epoch_stretch or epoch < min_epochs) and epoch < max_epochs and not killed:\n",
    "        net.train()\n",
    "        run_loss = 0.0\n",
    "        sum_loss = 0.0\n",
    "        count = 0\n",
    "        correct = 0.0\n",
    "        categorical_correct = [0.0 for i in range(num_classes)]\n",
    "        for index, data in enumerate(load_train, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # gather accuracy stats:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "            for i in range(num_classes):\n",
    "                categorical_correct[i] += ((predicted==i) == (labels==i)).float().sum().item()\n",
    "                #print(classes[i],categorical_correct[i]/((index+1)*batch_size))\n",
    "            \n",
    "            # print statistics\n",
    "            run_loss += loss.item()\n",
    "            sum_loss += loss.item()\n",
    "            count += 1 \n",
    "            if index % 200 == 0:    # print every 200 mini-batches\n",
    "                print('  Epoch: {} [{}/{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}%'.format(\n",
    "                    epoch, int(correct), (index + 1) * batch_size, len(load_train.dataset),\n",
    "                    100. * index / len(load_train), loss.item(), 100. * correct / ((index+1) * batch_size)))\n",
    "                run_loss = 0.0\n",
    "        accuracy = 100. * correct / len(load_train.dataset)\n",
    "        print('Epoch: {}\\tLoss: {:.6f}\\tAcc: {:.6f}'.format(\n",
    "                epoch, sum_loss/count, accuracy))\n",
    "        writer.add_scalar(\"Loss/train\", sum_loss/count, epoch)\n",
    "        writer.add_scalar(\"Acc/train\", accuracy, epoch)\n",
    "        \n",
    "        modules_list = iter(net.named_modules())\n",
    "        next(modules_list)\n",
    "        for module in modules_list:\n",
    "            try:\n",
    "                writer.add_histogram(\"Model/\"+module[0]+\".weights\", module[1].weight, epoch)\n",
    "                writer.add_histogram(\"Model/\"+module[0]+\".bias\", module[1].bias, epoch)\n",
    "            except:\n",
    "                pass\n",
    "        # Categorical accuracy:\n",
    "        for i in range(num_classes):\n",
    "            writer.add_scalar(\"Acc/\" + classes[i],categorical_correct[i]/len(load_train.dataset), epoch)\n",
    "        torch.save(net.state_dict(), SAVE_PATH + tag + \"-progress\")\n",
    "        \n",
    "        # get validation accuracy: \n",
    "        valid_acc, valid_loss = validate(net, device, load_valid, optimizer, criterion)\n",
    "        writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/valid\", valid_acc, epoch)\n",
    "        print('Validation: acc: {:.6f}%\\tloss: {:.6f}'.format(\n",
    "                valid_acc, valid_loss))\n",
    "        validation_accuracies.append(valid_acc)\n",
    "        best_epoch = validation_accuracies.index(max(validation_accuracies))\n",
    "        \n",
    "        # get test accuracy: \n",
    "        test_acc, test_loss = test(net, device, load_test, criterion)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/test\", test_acc, epoch)\n",
    "        print('Testing: acc: {:.6f}%\\tloss: {:.6f}'.format(\n",
    "                test_acc, test_loss))\n",
    "        \n",
    "        # this is the best epoch so far, save these weights:\n",
    "        if (best_epoch == epoch):\n",
    "            torch.save(net.state_dict(), SAVE_PATH + tag + \"-best\")\n",
    "        if (epoch >= 2):\n",
    "            if (validation_accuracies[epoch] == 100/num_classes):\n",
    "                killed = True\n",
    "                print(\"[!] This run has failed, accuracies are bad. Aborting.\")\n",
    "        \n",
    "        print('Best: {} @ {:.6f}% -> epoch target {}'.format(best_epoch,validation_accuracies[best_epoch],max([best_epoch+epoch_stretch,min_epochs])))\n",
    "        epoch += 1\n",
    "        \n",
    "    print('Done training.')\n",
    "    torch.save(net.state_dict(), SAVE_PATH + tag + \"-final\")\n",
    "    \n",
    "    ## TODO: test here\n",
    "\n",
    "    prediction_list = torch.zeros(0,dtype=torch.long).to(device)\n",
    "    label_list = torch.zeros(0,dtype=torch.long).to(device)\n",
    "    \n",
    "    ## Testing\n",
    "    net.eval().to(device)\n",
    "    correct = 0\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(load_test, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            valid_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            prediction_list = torch.cat([prediction_list, predicted.view(-1)])  \n",
    "            label_list = torch.cat([label_list, labels.view(-1)])\n",
    "            #print(\"Predictions, ground:\")\n",
    "            #print(prediction_list)\n",
    "            #print(label_list)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "    print(\" Testing correct: {} / {}\".format(correct,len(load_test.dataset)))        \n",
    "    t_acc = 100 * correct / len(load_test.dataset)\n",
    "    t_loss = valid_loss / len(load_test.dataset)\n",
    "\n",
    "    \n",
    "    matrix = confusion_matrix(label_list.cpu().numpy(), prediction_list.cpu().numpy())\n",
    "    print(matrix)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(matrix)\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(classes)))\n",
    "    ax.set_yticks(np.arange(len(classes)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticklabels(classes)\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            text = ax.text(j, i, \"{:4.2f}\".format(matrix[i, j]/sum(matrix[i])),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Testing confusion matrix (n = {})\".format(len(load_test.dataset)))\n",
    "\n",
    "    plt.show()\n",
    "    writer.add_figure('Testing/conf',fig)\n",
    "    \n",
    "    ## Visualise features\n",
    "    FV = FilterVisualizer(net,VISUAL_OUT)\n",
    "    image_out = reconstructions_single_layer((net.children())[4][0].conv1,'Layer 1 Block 1 Conv1',\n",
    "                                             list(range(6,12)),n_cols=3,\n",
    "                                             save_fig=True,album_hash=None)\n",
    "    \n",
    "    class_accuracy=100*matrix.diagonal() / matrix.sum(1)\n",
    "    #print(classes)\n",
    "    #print(class_accuracy)\n",
    "    for i in range(len(classes)):\n",
    "        print(\"{}: {:.4f}\".format(classes[i],class_accuracy[i]))\n",
    "    print(\"Best val_acc: {:6.4f}\".format(max(validation_accuracies)))\n",
    "    return max(validation_accuracies)\n",
    "\n",
    "# magic here.\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        run_writer = SummaryWriter(log_dir = run_dir)            \n",
    "        if (hparams[HP_YOLOCROPPED]):\n",
    "            train_path = ABS_PATH_TRAIN\n",
    "            valid_path = ABS_PATH_VALID\n",
    "            test_path = ABS_PATH_TEST\n",
    "            print(\"Running using cropped (yolo) images\")\n",
    "        else:\n",
    "            train_path = ABS_PATH_TRAIN_RAW\n",
    "            valid_path = ABS_PATH_VALID_RAW\n",
    "            test_path = ABS_PATH_TEST_RAW\n",
    "            print(\"Running using uncropped (plain) images\")\n",
    "        \n",
    "        accuracy = train(max_epochs = 100, min_epochs = 3, epoch_stretch = 1, batch_size = 32, train_path = train_path, valid_path = valid_path, test_path = test_path, labels = classes, hparams = hparams, writer = run_writer)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "if (skip_all is False): \n",
    "    HP_NUM_UNITS = hp.HParam('channels', hp.Discrete([128, 256]))\n",
    "    HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "    #HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['sgd','adagrad']))\n",
    "    HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['sgd']))\n",
    "    HP_YOLOCROPPED = hp.HParam('processed (yolo)', hp.Discrete([True, False]))\n",
    "    METRIC_ACCURACY = 'accuracy'\n",
    "    \n",
    "    hpdirname = 'runs/' + (datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + ':hparam_tuning')\n",
    "    with tf.summary.create_file_writer(hpdirname).as_default():\n",
    "        hp.hparams_config(\n",
    "            hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_YOLOCROPPED],\n",
    "            metrics=[hp.Metric(METRIC_ACCURACY, display_name='best accuracy (validation)')]\n",
    "        )\n",
    "    \n",
    "    session_num = 0\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "            for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                for yolo in HP_YOLOCROPPED.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_NUM_UNITS: num_units,\n",
    "                        HP_DROPOUT: float(\"%0.2f\"%float(dropout_rate)),\n",
    "                        HP_OPTIMIZER: optimizer,\n",
    "                        HP_YOLOCROPPED: yolo,\n",
    "                    }\n",
    "                    torch.cuda.empty_cache()\n",
    "                    run_name = \"run-%d\" % session_num\n",
    "                    print(\"-> Starting trial %s\" % run_name)\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    if hparams[HP_YOLOCROPPED]:\n",
    "                        run(os.path.join(hpdirname, '_' + str(session_num) + '_true'), hparams)\n",
    "                    else:\n",
    "                        run(os.path.join(hpdirname, '_' + str(session_num)) + '_false', hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 stage classifier\n",
    "\n",
    "1. Train to detect *Apis Mellifera* vs. *Bombus Auricomus* vs. a merged set of the other classes.\n",
    "2. Train a second model to tell apart the other three classes.'\n",
    "3. Note that both of these datasets will by default be unbalanced unless something is changed above.\n",
    "4. When running detections, if the first model's confidence is below a threshold, apply the second model to see if there is an improved prediction.\n",
    "\n",
    "`todo: balance datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os, datetime, math\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "length = 512\n",
    "var_droupout = 0.2\n",
    "skip_all = True\n",
    "tag1 = ':' + str(length) + '_STAGE-1_' + str(var_droupout)\n",
    "tag2 = ':' + str(length) + '_STAGE-2_' + str(var_droupout)\n",
    "writer1 = SummaryWriter(log_dir = os.path.join('runs/',(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + tag1)))\n",
    "writer2 = SummaryWriter(log_dir = os.path.join('runs/',(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + tag2)))\n",
    "print(\"tensorboard writing\"+tag1)\n",
    "print(\"tensorboard writing\"+tag2)\n",
    "\n",
    "classes1 = ['Apis_mellifera','Bombus_impatiens','Merged']\n",
    "classes2 = ['Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n",
    "\n",
    "ABS_PATH_TRAIN1 = '/m2docs/res/data1/train'\n",
    "ABS_PATH_VALID1 = '/m2docs/res/data1/valid'\n",
    "ABS_PATH_TEST1 = '/m2docs/res/data1/test'\n",
    "ABS_PATH_TRAIN2 = '/m2docs/res/data2/train'\n",
    "ABS_PATH_VALID2 = '/m2docs/res/data2/valid'\n",
    "ABS_PATH_TEST2 = '/m2docs/res/data2/test'\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def load(dir_name, batch_size, shuffle = False):\n",
    "    return(\n",
    "        #create a data loader\n",
    "        torch.utils.data.DataLoader(\n",
    "            datasets.ImageFolder(root = dir_name, transform = transforms.ToTensor()),\n",
    "            batch_size = batch_size,\n",
    "            num_workers = 2,\n",
    "            shuffle = shuffle\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Layer/network 1:\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout2d(p = var_droupout)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*28*28, length)\n",
    "        self.fc2 = nn.Linear(length, int(length/2))\n",
    "        #self.fc3 = nn.Linear(int(length/2), int(length/4))\n",
    "        self.fc3 = nn.Linear(int(length/2), 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv2(x))))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv3(x))))\n",
    "        x = (F.relu(self.pool(self.conv4(x))))\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "# Layer/network 2:    \n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout2d(p = var_droupout)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*28*28, length)\n",
    "        self.fc2 = nn.Linear(length, int(length/2))\n",
    "        #self.fc3 = nn.Linear(int(length/2), int(length/4))\n",
    "        self.fc3 = nn.Linear(int(length/2), 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv2(x))))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv3(x))))\n",
    "        x = (F.relu(self.pool(self.conv4(x))))\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "def validate(network,device,load_valid,criterion = nn.CrossEntropyLoss()):\n",
    "    network.eval().to(device)\n",
    "    correct = 0\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(load_valid, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = network(inputs)\n",
    "            valid_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # gather accuracy stats:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "    accuracy = 100 * correct / len(load_valid.dataset)\n",
    "    valid_loss = valid_loss / len(load_valid.dataset)\n",
    "    return accuracy , valid_loss\n",
    "\n",
    "def train(max_epochs = 50, min_epochs = 5, epoch_stretch = 5, train_path = ABS_PATH_TRAIN, valid_path = ABS_PATH_VALID, test_path = ABS_PATH_TEST, labels = classes, batch_size = 24):\n",
    "    epochs = max_epochs\n",
    "    class_names = labels\n",
    "    num_classes = len(class_names)\n",
    "    train_batch = batch_size\n",
    "    test_batch = 128\n",
    "    SAVE_PATH = '/m2docs/res/models'\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device: {}\".format(device))\n",
    "    net = Net().to(device)\n",
    "    print(net)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "    torch.manual_seed(417)\n",
    "    \n",
    "    load_train = load(train_path, batch_size, shuffle=True)\n",
    "    load_valid = load(valid_path, batch_size, shuffle=True)\n",
    "    load_test  = load(test_path, batch_size, shuffle=True)\n",
    "\n",
    "    validation_accuracies = []\n",
    "    best_epoch = 0\n",
    "    epoch = 0\n",
    "    while (epoch <= best_epoch + epoch_stretch or epoch < min_epochs) and epoch < max_epochs:\n",
    "        net.train()\n",
    "        run_loss = 0.0\n",
    "        sum_loss = 0.0\n",
    "        count = 0\n",
    "        correct = 0.0\n",
    "        categorical_correct = [0.0 for i in range(num_classes)]\n",
    "        for index, data in enumerate(load_train, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # gather accuracy stats:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "            for i in range(num_classes):\n",
    "                categorical_correct[i] += ((predicted==i) == (labels==i)).float().sum().item()\n",
    "                #print(classes[i],categorical_correct[i]/((index+1)*batch_size))\n",
    "            \n",
    "            # print statistics\n",
    "            run_loss += loss.item()\n",
    "            sum_loss += loss.item()\n",
    "            count += 1 \n",
    "            if index % 200 == 0:    # print every 200 mini-batches\n",
    "                print('  Epoch: {} [{}/{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}%'.format(\n",
    "                    epoch, int(correct), (index + 1) * batch_size, len(load_train.dataset),\n",
    "                    100. * index / len(load_train), loss.item(), 100. * correct / ((index+1) * batch_size)))\n",
    "                run_loss = 0.0\n",
    "        accuracy = 100. * correct / len(load_train.dataset)\n",
    "        print('Epoch: {}\\tLoss: {:.6f}\\tAcc: {:.6f}'.format(\n",
    "                epoch, sum_loss/count, accuracy))\n",
    "        writer.add_scalar(\"Loss/train\", sum_loss/count, epoch)\n",
    "        writer.add_scalar(\"Acc/train\", accuracy, epoch)\n",
    "        \n",
    "        modules_list = iter(net.named_modules())\n",
    "        next(modules_list)\n",
    "        for module in modules_list:\n",
    "            try:\n",
    "                writer.add_histogram(\"Model/\"+module[0]+\".weights\", module[1].weight, epoch)\n",
    "                writer.add_histogram(\"Model/\"+module[0]+\".bias\", module[1].bias, epoch)\n",
    "            except:\n",
    "                pass\n",
    "        # Categorical accuracy:\n",
    "        for i in range(num_classes):\n",
    "            writer.add_scalar(\"Acc/\" + classes[i],categorical_correct[i]/len(load_train.dataset), epoch)\n",
    "        torch.save(net.state_dict(), SAVE_PATH+\"_progress\")\n",
    "        \n",
    "        # get validation accuracy: \n",
    "        valid_acc, valid_loss = validate(net, device, load_valid, criterion)\n",
    "        writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/valid\", valid_acc, epoch)\n",
    "        print('Validation: acc: {:.6f}%\\tloss: {:.6f}'.format(\n",
    "                valid_acc, valid_loss))\n",
    "        validation_accuracies.append(valid_acc)\n",
    "        best_epoch = validation_accuracies.index(max(validation_accuracies))\n",
    "        print('Best: {} @ {:.6f}% -> epoch target {}'.format(best_epoch,validation_accuracies[best_epoch],max([best_epoch+epoch_stretch,min_epochs])))\n",
    "        epoch += 1\n",
    "        \n",
    "    print('Done training.')\n",
    "    torch.save(net.state_dict(), SAVE_PATH)\n",
    "\n",
    "    prediction_list = torch.zeros(0,dtype=torch.long).to(device)\n",
    "    label_list = torch.zeros(0,dtype=torch.long).to(device)\n",
    "              \n",
    "    with torch.no_grad():\n",
    "        for data in load_test:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "              \n",
    "            prediction_list = torch.cat([prediction_list, predicted.view(-1)])  \n",
    "            label_list = torch.cat([label_list, labels.view(-1)])\n",
    "    \n",
    "    matrix = confusion_matrix(label_list.cpu().numpy(), prediction_list.cpu().numpy())\n",
    "    print(matrix)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(matrix)\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(classes)))\n",
    "    ax.set_yticks(np.arange(len(classes)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticklabels(classes)\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            text = ax.text(j, i, matrix[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Testing confusion matrix\")\n",
    "\n",
    "    plt.show()\n",
    "    writer.add_figure('Testing/conf',fig)\n",
    "    \n",
    "    class_accuracy=100*matrix.diagonal() / matrix.sum(1)\n",
    "    print(classes)\n",
    "    print(class_accuracy)\n",
    "\n",
    "if (skip_all is False):\n",
    "    # setup: create 2-stage directories:\n",
    "    # data1: first stage: mellifera, impatiens, 3-merged\n",
    "    %mkdir data1\n",
    "    %rm -r data1/*\n",
    "    %mkdir data1/train\n",
    "    %cp -r data/train/Apis_mellifera data1/train/Apis_mellifera\n",
    "    %cp -r data/train/Bombus_auricomus data1/train/Bombus_auricomus\n",
    "    %cp -r data/train/Bombus_bimaculatus data1/train/Merged\n",
    "    %cp data/train/Bombus_griseocollis/* data1/train/Merged\n",
    "    %cp data/train/Bombus_impatiens/* data1/train/Merged\n",
    "    %mkdir data1/valid\n",
    "    %cp -r data/valid/Apis_mellifera data1/valid/Apis_mellifera\n",
    "    %cp -r data/valid/Bombus_auricomus data1/valid/Bombus_auricomus\n",
    "    %cp -r data/valid/Bombus_bimaculatus data1/valid/Merged\n",
    "    %cp data/valid/Bombus_griseocollis/* data1/valid/Merged\n",
    "    %cp data/valid/Bombus_impatiens/* data1/valid/Merged\n",
    "    %mkdir data1/test\n",
    "    %cp -r data/test/Apis_mellifera data1/test/Apis_mellifera\n",
    "    %cp -r data/test/Bombus_auricomus data1/test/Bombus_auricomus\n",
    "    %cp -r data/test/Bombus_bimaculatus data1/test/Merged\n",
    "    %cp data/test/Bombus_griseocollis/* data1/test/Merged\n",
    "    %cp data/test/Bombus_impatiens/* data1/test/Merged\n",
    "\n",
    "    # data2: auricomus vs bimaculatus vs griseocollis\n",
    "    %mkdir data2\n",
    "    %rm -r data2/*\n",
    "    %mkdir data2/train\n",
    "    %cp -r data/train/Bombus_bimaculatus data2/train/Bombus_bimaculatus\n",
    "    %cp -r data/train/Bombus_griseocollis data2/train/Bombus_griseocollis\n",
    "    %cp -r data/train/Bombus_impatiens data2/train/Bombus_impatiens\n",
    "    %mkdir data2/valid\n",
    "    %cp -r data/valid/Bombus_bimaculatus data2/valid/Bombus_bimaculatus\n",
    "    %cp -r data/valid/Bombus_griseocollis data2/valid/Bombus_griseocollis\n",
    "    %cp -r data/valid/Bombus_impatiens data2/valid/Bombus_impatiens\n",
    "    %mkdir data2/test\n",
    "    %cp -r data/test/Bombus_bimaculatus data2/test/Bombus_bimaculatus\n",
    "    %cp -r data/test/Bombus_griseocollis data2/test/Bombus_griseocollis\n",
    "    %cp -r data/test/Bombus_impatiens data2/test/Bombus_impatiens\n",
    "    \n",
    "    # train\n",
    "    train(max_epochs = 160, min_epochs = 70, epoch_stretch = 15, train_path = ABS_PATH_TRAIN, valid_path = ABS_PATH_VALID, test_path = ABS_PATH_TEST, labels = classes, batch_size = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
