{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "### Step 1: resize/crop images\n",
    "Requires having run: `beescrape.py`\n",
    "Depends on yolov5 and `shutil`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resizing images to (512,512) and copying uncropped = True\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "import os\n",
    "from os import listdir\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import skimage\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = '/home/wilber/Documents/RESEARCH/research/beespotter'\n",
    "OUT_DIR = '/m2docs/res/data'\n",
    "CROPPED_PATH = '/m2docs/res/cropped_imgs'\n",
    "cropped_files = listdir(CROPPED_PATH)\n",
    "\n",
    "# Args:\n",
    "VAL_SIZE = 256\n",
    "TEST_SIZE = 128\n",
    "preprocess = True\n",
    "resize = True\n",
    "length = 512\n",
    "size = (length, length) # (512, 512)\n",
    "\n",
    "# Uncropped args: (use the same images except without passing through yolo bee finder for comparison)\n",
    "OUT_DIR_RAW = '/m2docs/res/data_raw'\n",
    "UNCROPPED_PATH = '/m2docs/res/uncropped_imgs'\n",
    "COPY_UNCROPPED = True\n",
    "\n",
    "print(\"resizing images to ({},{}) and copying uncropped = {}\".format(length,length,COPY_UNCROPPED))\n",
    "\n",
    "# Species with >= 1000 images:\n",
    "classes = ['Apis_mellifera','Bombus_impatiens','Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2757 [00:00<06:56,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3545-4.jpg -33 salt 1 RGB\n",
      "3545-4.jpg size: 3072x4608x3 mode RGB\n",
      "3545-4.jpg preds: 1334, 2210, 927, 927\n",
      "Bounds x 407-2261 y 1283-3137\n",
      "  3545-4.jpg -19 salt 2 RGB\n",
      "3545-4.jpg size: 3072x4608x3 mode RGB\n",
      "3545-4.jpg preds: 1334, 2210, 927, 927\n",
      "Bounds x 407-2261 y 1283-3137\n",
      "  3545-4.jpg 108 None 3 RGB\n",
      "3545-4.jpg size: 3072x4608x3 mode RGB\n",
      "3545-4.jpg preds: 1334, 2210, 927, 927\n",
      "Bounds x 407-2261 y 1283-3137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2757 [00:01<10:03,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  458-2.jpg 1 salt 1 RGB\n",
      "458-2.jpg size: 301x373x3 mode RGB\n",
      "458-2.jpg preds: 138, 231, 167, 167\n",
      "Bounds x -29-305 y 64-398\n",
      "Using old cropper (img too small)\n",
      "  458-2.jpg -7 s&p 2 RGB\n",
      "458-2.jpg size: 301x373x3 mode RGB\n",
      "458-2.jpg preds: 138, 231, 167, 167\n",
      "Bounds x -29-305 y 64-398\n",
      "Using old cropper (img too small)\n",
      "  458-2.jpg 7 salt 3 RGB\n",
      "458-2.jpg size: 301x373x3 mode RGB\n",
      "458-2.jpg preds: 138, 231, 167, 167\n",
      "Bounds x -29-305 y 64-398\n",
      "Using old cropper (img too small)\n",
      "  7975-2.jpg -15 s&p 1 RGB\n",
      "7975-2.jpg size: 3456x5184x3 mode RGB\n",
      "7975-2.jpg preds: 1380, 3059, 917, 917\n",
      "Bounds x 463-2297 y 2142-3976\n",
      "  7975-2.jpg 24 pepper 2 RGB\n",
      "7975-2.jpg size: 3456x5184x3 mode RGB\n",
      "7975-2.jpg preds: 1380, 3059, 917, 917\n",
      "Bounds x 463-2297 y 2142-3976\n",
      "  7975-2.jpg 306 s&p 3 RGB\n",
      "7975-2.jpg size: 3456x5184x3 mode RGB\n",
      "7975-2.jpg preds: 1380, 3059, 917, 917\n",
      "Bounds x 463-2297 y 2142-3976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 8/2757 [00:02<22:34,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1608-1.jpg 39 None 1 RGB\n",
      "1608-1.jpg size: 1015x651x3 mode RGB\n",
      "1608-1.jpg preds: 780, 305, 592, 592\n",
      "Bounds x 188-1372 y -287-897\n",
      "Using old cropper (img too small)\n",
      "  1608-1.jpg 9 None 2 RGB\n",
      "1608-1.jpg size: 1015x651x3 mode RGB\n",
      "1608-1.jpg preds: 780, 305, 592, 592\n",
      "Bounds x 188-1372 y -287-897\n",
      "Using old cropper (img too small)\n",
      "  1608-1.jpg 251 None 3 RGB\n",
      "1608-1.jpg size: 1015x651x3 mode RGB\n",
      "1608-1.jpg preds: 780, 305, 592, 592\n",
      "Bounds x 188-1372 y -287-897\n",
      "Using old cropper (img too small)\n",
      "  5822-2.jpg 0 pepper 1 RGB\n",
      "5822-2.jpg size: 664x632x3 mode RGB\n",
      "5822-2.jpg preds: 341, 326, 132, 132\n",
      "Bounds x 209-473 y 194-458\n",
      "  5822-2.jpg 57 s&p 2 RGB\n",
      "5822-2.jpg size: 664x632x3 mode RGB\n",
      "5822-2.jpg preds: 341, 326, 132, 132\n",
      "Bounds x 209-473 y 194-458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 14/2757 [00:03<16:29,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5822-2.jpg 250 s&p 3 RGB\n",
      "5822-2.jpg size: 664x632x3 mode RGB\n",
      "5822-2.jpg preds: 341, 326, 132, 132\n",
      "Bounds x 209-473 y 194-458\n",
      "  7917-1.jpg 33 s&p 1 RGB\n",
      "7917-1.jpg size: 3024x4032x3 mode RGB\n",
      "7917-1.jpg preds: 2177, 1762, 614, 614\n",
      "Bounds x 1563-2791 y 1148-2376\n",
      "  7917-1.jpg 46 s&p 2 RGB\n",
      "7917-1.jpg size: 3024x4032x3 mode RGB\n",
      "7917-1.jpg preds: 2177, 1762, 614, 614\n",
      "Bounds x 1563-2791 y 1148-2376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 17/2757 [00:03<15:09,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7917-1.jpg 101 None 3 RGB\n",
      "7917-1.jpg size: 3024x4032x3 mode RGB\n",
      "7917-1.jpg preds: 2177, 1762, 614, 614\n",
      "Bounds x 1563-2791 y 1148-2376\n",
      "  6043-1.jpg -33 s&p 1 RGB\n",
      "6043-1.jpg size: 3456x4608x3 mode RGB\n",
      "6043-1.jpg preds: 1823, 2289, 764, 764\n",
      "Bounds x 1059-2587 y 1525-3053\n",
      "  6043-1.jpg -41 s&p 2 RGB\n",
      "6043-1.jpg size: 3456x4608x3 mode RGB\n",
      "6043-1.jpg preds: 1823, 2289, 764, 764\n",
      "Bounds x 1059-2587 y 1525-3053\n",
      "  6043-1.jpg 155 None 3 RGB\n",
      "6043-1.jpg size: 3456x4608x3 mode RGB\n",
      "6043-1.jpg preds: 1823, 2289, 764, 764\n",
      "Bounds x 1059-2587 y 1525-3053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/2757 [00:05<10:51,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3136-1.jpg -39 s&p 1 RGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d14ef0a85a26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             \u001b[0mrotimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mrotimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                                 \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNOISE_AMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-d14ef0a85a26>\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(filename, angle)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mimg_dim_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dim_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} size: {}x{}x{} mode {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_dim_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_dim_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 0)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "from IPython.display import display\n",
    "\n",
    "TXT_DIR = '/m2docs/res/predicted_text/' # text directory\n",
    "IMG_DIR = '/m2docs/res/bees_unsorted_all' # images directory\n",
    "\n",
    "%rm -R /m2docs/res/data/*\n",
    "%rm -R /m2docs/res/data_raw/*\n",
    "\n",
    "VAL_SIZE = 2\n",
    "TEST_SIZE = 2\n",
    "NEW_ROTATE = True\n",
    "# print(\"resizing images to {}\".format(size))\n",
    "\n",
    "def rotate(filename, angle):\n",
    "    if NEW_ROTATE is True:\n",
    "        pic = Image.open(join(IMG_DIR,filename))\n",
    "        img = np.asarray(pic)\n",
    "        [img_dim_x, img_dim_y, z] = img.shape\n",
    "        print(\"{} size: {}x{}x{} mode {}\".format(filename,img_dim_x,img_dim_y,z,pic.mode))\n",
    "\n",
    "        # Get prediction\n",
    "        if (isfile(join(TXT_DIR,filename.replace('.jpg','.txt')))):\n",
    "            inference = open(join(TXT_DIR,filename.replace('.jpg','.txt')),\"r\")\n",
    "\n",
    "            for line in inference:\n",
    "                [id, x, y, xlen, ylen] = [float(i) for i in line.split()]\n",
    "\n",
    "                box_center_x = int(y*img_dim_x)\n",
    "                box_center_y = int(x*img_dim_y)\n",
    "                box_length\t = int(xlen*img_dim_x)\n",
    "                box_height\t = int(ylen*img_dim_y)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        edge_length = 0\n",
    "        edge_length = max(box_length, box_height)\n",
    "        \n",
    "        print(\"{} preds: {}, {}, {}, {}\".format(filename, box_center_x,box_center_y,edge_length,edge_length))\n",
    "        (x1, x2, y1, y2) = (int(box_center_x - edge_length),\n",
    "                            int(box_center_x + edge_length), \n",
    "                            int(box_center_y - edge_length),\n",
    "                            int(box_center_y + edge_length))\n",
    "        print(\"Bounds x {}-{} y {}-{}\".format(x1,x2,y1,y2))\n",
    "\n",
    "        if x1 < 0: x1 = 0\n",
    "        if y1 < 0: y1 = 0\n",
    "        if x2 > img_dim_x: x2 = img_dim_x\n",
    "        if y2 > img_dim_y: y2 = img_dim_y\n",
    "\n",
    "        im_arr = np.asarray(img)[x1:x2, y1:y2]\n",
    "        \n",
    "        if img_dim_x < edge_length*2 or img_dim_y < edge_length*2:\n",
    "            # Just do the old version then\n",
    "            print(\"Using old cropper (img too small)\")\n",
    "            pic = Image.open(os.path.join(CROPPED_PATH,filename))\n",
    "            return pic.rotate(angle)\n",
    "        \n",
    "        rotated_image = np.asarray(Image.fromarray(im_arr,pic.mode).rotate(angle))\n",
    "        crop_rot = rotated_image[rotated_image.shape[0]//4:\n",
    "                            3*rotated_image.shape[0]//4,\n",
    "                            rotated_image.shape[0]//4:\n",
    "                            3*rotated_image.shape[0]//4]\n",
    "        img = Image.fromarray(crop_rot,pic.mode).resize(size)\n",
    "        return img\n",
    "    else:\n",
    "        pic = Image.open(os.path.join(CROPPED_PATH,filename))\n",
    "        return pic.rotate(angle)\n",
    "\n",
    "# from https://gist.github.com/Prasad9/28f6a2df8e8d463c6ddd040f4f6a028a\n",
    "noise_modes = [None,'salt','pepper','s&p']\n",
    "NOISE_AMOUNT = .01 # default amount\n",
    "def add_noise(img, mode, noise_amount = .03):\n",
    "    if mode is not None:\n",
    "        gimg = skimage.util.random_noise(img, mode = mode, amount = random.uniform(0,noise_amount))\n",
    "        return gimg\n",
    "    else:\n",
    "        print(\"oops, you shouldn't see this\")\n",
    "\n",
    "total_skipped = 0\n",
    "for label in classes:  # for each type of bee\n",
    "    skipnum = 0\n",
    "    classID = classes.index(label)\n",
    "    \n",
    "    input_path = os.path.join(DATA_DIR, label)\n",
    "    im_list = os.listdir(input_path)\n",
    "    random.shuffle(im_list)\n",
    "    \n",
    "    # Attempt to create directories:\n",
    "    for a in [OUT_DIR,OUT_DIR_RAW]:\n",
    "        for b in ['/train/','/test/','/valid/']:\n",
    "            if not os.path.exists(a+b+label):\n",
    "                os.makedirs(a+b+label)\n",
    "        \n",
    "    train_path = os.path.join(OUT_DIR,'train/'+ label + '/')\n",
    "    test_path  = os.path.join(OUT_DIR,'test/' + label + '/')\n",
    "    valid_path = os.path.join(OUT_DIR,'valid/' + label + '/')\n",
    "    train_path_raw = os.path.join(OUT_DIR_RAW,'train/'+ label + '/')\n",
    "    test_path_raw  = os.path.join(OUT_DIR_RAW,'test/' + label + '/')\n",
    "    valid_path_raw = os.path.join(OUT_DIR_RAW,'valid/' + label + '/')\n",
    "    \n",
    "    index = 0\n",
    "    for img in tqdm(im_list):\n",
    "        if img in cropped_files:\n",
    "            pic = Image.open(os.path.join(CROPPED_PATH,img))\n",
    "            \n",
    "            if (COPY_UNCROPPED):\n",
    "                    pic2 = Image.open(os.path.join(UNCROPPED_PATH,img))\n",
    "            \n",
    "            if resize: #resize image\n",
    "                out = pic.resize(size)\n",
    "                if (COPY_UNCROPPED):\n",
    "                    out2 = pic2.resize(size)\n",
    "            else:\n",
    "                out = pic\n",
    "                if (COPY_UNCROPPED):\n",
    "                    out2 = pic2\n",
    "            \n",
    "            if index < VAL_SIZE:\n",
    "                out.save(os.path.join(valid_path,img))\n",
    "            elif index < VAL_SIZE + TEST_SIZE:\n",
    "                out.save(os.path.join(test_path,img))\n",
    "            if index < VAL_SIZE and COPY_UNCROPPED:\n",
    "                out2.save(os.path.join(valid_path_raw,img))\n",
    "            elif index < VAL_SIZE + TEST_SIZE and COPY_UNCROPPED:\n",
    "                out2.save(os.path.join(test_path_raw,img))    \n",
    "            else:    #training set, rotate\n",
    "                out.save(os.path.join(train_path,img.replace('.jpg','-0.jpg')))\n",
    "                if COPY_UNCROPPED:\n",
    "                    out2.save(os.path.join(train_path_raw,img.replace('.jpg','-0.jpg')))\n",
    "                if preprocess:\n",
    "                    for rot,mode,ext in zip([random.randint(-44,45),random.randint(-90,90),random.randint(0,359)],[random.choice(noise_modes),random.choice(noise_modes),random.choice(noise_modes)],[1,2,3]):\n",
    "                        print(' ',img,rot,mode,ext,pic.mode)\n",
    "                        if mode is not None:\n",
    "                            rotimg = rotate(img,rot)\n",
    "                            if rotimg is not None:\n",
    "                                Image.fromarray((add_noise(np.array(rotimg),mode,NOISE_AMOUNT)*255).astype(np.uint8),pic.mode).save(os.path.join(train_path,img.replace('.jpg','-'+str(ext)+'.jpg')))\n",
    "                            else:\n",
    "                                print(\"Rotation failed.\")\n",
    "                            if COPY_UNCROPPED:\n",
    "                                Image.fromarray((add_noise(np.array(out2.rotate(rot)),mode,NOISE_AMOUNT)*255).astype(np.uint8),pic2.mode).save(os.path.join(train_path_raw,img.replace('.jpg','-'+str(ext)+'.jpg')))\n",
    "                        else:\n",
    "                            rotimg = rotate(img,rot)\n",
    "                            if rotimg is not None:\n",
    "                                rotimg.save(os.path.join(train_path,img.replace('.jpg','-'+str(ext)+'.jpg')))\n",
    "                            else:\n",
    "                                print(\"Rotation failed.\")\n",
    "                            if COPY_UNCROPPED:\n",
    "                                out2.rotate(rot).save(os.path.join(train_path_raw,img.replace('.jpg','-'+str(ext)+'.jpg')))\n",
    "                \n",
    "        else:\n",
    "            skipnum += 1\n",
    "            index -= 1\n",
    "            pass\n",
    "        index += 1\n",
    "#     print(label, \"skipped\", skipnum)\n",
    "#     print(label, \"total\", index)\n",
    "    \n",
    "    total_skipped += skipnum\n",
    "    \n",
    "print(\"Images moved and rotated. Skipped \", total_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apis_mellifera\n",
      "images in /train/: 3332\n",
      "images in /train/: 3332 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "Bombus_impatiens\n",
      "images in /train/: 6532\n",
      "images in /train/: 6532 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "Bombus_auricomus\n",
      "images in /train/: 1360\n",
      "images in /train/: 1360 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "Bombus_bimaculatus\n",
      "images in /train/: 2908\n",
      "images in /train/: 2908 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "Bombus_griseocollis\n",
      "images in /train/: 5788\n",
      "images in /train/: 5788 (uncropped)\n",
      "images in /test/: 128\n",
      "images in /test/: 128 (uncropped)\n",
      "images in /valid/: 256\n",
      "images in /valid/: 256 (uncropped)\n",
      "[3332, 6532, 1360, 2908, 5788]\n",
      "[3332, 6532, 1360, 2908, 5788]\n"
     ]
    }
   ],
   "source": [
    "import fnmatch\n",
    "cropped_count = []\n",
    "uncropped_count = []\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    cropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR + '/train/' + c + \"/\")), '*.jpg')))\n",
    "    uncropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR_RAW + '/train/' + c + \"/\")), '*.jpg')))\n",
    "    for t in ['/train/','/test/','/valid/']:\n",
    "        print(\"images in {}: {}\".format(t,len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR + t + c + \"/\")), '*.jpg'))))\n",
    "        print(\"images in {}: {} (uncropped)\".format(t,len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR_RAW + t + c + \"/\")), '*.jpg'))))\n",
    "print(cropped_count)\n",
    "print(uncropped_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apis_mellifera\n",
      "Bombus_impatiens\n",
      "Bombus_auricomus\n",
      "Bombus_bimaculatus\n",
      "Bombus_griseocollis\n",
      "['Apis_mellifera', 'Bombus_impatiens', 'Bombus_auricomus', 'Bombus_bimaculatus', 'Bombus_griseocollis']\n",
      "[3332, 6532, 1360, 2908, 5788]\n",
      "[3332, 6532, 1360, 2908, 5788]\n",
      "Target number of images: 7185.200000000001\n",
      "Copying cropped images...\n",
      "[7186, 7186, 7186, 7186, 7186]\n",
      "Done.\n",
      "\n",
      "Copying uncropped images...\n",
      "['Apis_mellifera', 'Bombus_impatiens', 'Bombus_auricomus', 'Bombus_bimaculatus', 'Bombus_griseocollis']\n",
      "[3332, 6532, 1360, 2908, 5788]\n",
      "Target count: n = 7185.200000000001\n",
      "[7186, 7186, 7186, 7186, 7186]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Flip images over axes to even out training sets:\n",
    "import fnmatch\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import skimage\n",
    "\n",
    "classes = ['Apis_mellifera','Bombus_impatiens','Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n",
    "ABS_PATH_TRAIN = '/m2docs/res/data/train'\n",
    "MULTIPLIER = 1.05 # how many times more images need to be created: mult * len(largest class)\n",
    "verbose = False\n",
    "\n",
    "cropped_count = []\n",
    "uncropped_count = []\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    cropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR + '/train/' + c + \"/\")), '*.jpg')))\n",
    "    uncropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR_RAW + '/train/' + c + \"/\")), '*.jpg')))\n",
    "\n",
    "print(classes)\n",
    "print(cropped_count)\n",
    "print(uncropped_count)\n",
    "\n",
    "# add more images to largest class,\n",
    "# and bring the others to the same count\n",
    "target = max(cropped_count) * 1.1\n",
    "print(\"Target number of images: {}\".format(target))\n",
    "    \n",
    "print(\"Copying cropped images...\")\n",
    "\n",
    "noise_modes = [None,'salt','pepper','s&p']\n",
    "NOISE_AMOUNT = .01 # default amount\n",
    "def add_noise(img, mode, noise_amount = .03):\n",
    "    if mode is not None:\n",
    "        gimg = skimage.util.random_noise(img, mode = mode, amount = random.uniform(0, noise_amount))\n",
    "        return gimg\n",
    "    else:\n",
    "        print(\"oops, you shouldn't see this\")\n",
    "\n",
    "for c_name in classes:\n",
    "    current = cropped_count[classes.index(c_name)]\n",
    "    if (verbose): \n",
    "        print(c_name, current, \"->\", target)\n",
    "    im_list = os.listdir(os.path.join(ABS_PATH_TRAIN, c_name))\n",
    "    while current < target:\n",
    "        filename = random.choice(im_list)\n",
    "        if(verbose):\n",
    "            print(filename)\n",
    "        out = Image.open(os.path.join(ABS_PATH_TRAIN, c_name, filename))\n",
    "        \n",
    "        rot = random.randint(0,359)\n",
    "        mode = random.choice(noise_modes)\n",
    "        if (verbose): \n",
    "            print(filename,rot,mode,out.mode)\n",
    "        if (verbose): \n",
    "            print(filename.replace('.jpg','-x'+str(current)+'.jpg'))\n",
    "        if mode is not None:\n",
    "            Image.fromarray((add_noise(np.array(out.rotate(rot)),mode,NOISE_AMOUNT)*255).astype(np.uint8),pic.mode).save(os.path.join(ABS_PATH_TRAIN, c_name, filename.replace('.jpg','-x'+str(current)+'.jpg')))\n",
    "        else:\n",
    "            out.rotate(rot).save(os.path.join(ABS_PATH_TRAIN, c_name, filename.replace('.jpg','-x'+str(current)+'.jpg')))\n",
    "        current += 1\n",
    "\n",
    "cropped_count = []\n",
    "for c in classes:\n",
    "    cropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR + '/train/' + c + \"/\")), '*.jpg')))\n",
    "print(cropped_count)\n",
    "print(\"Done.\\n\")\n",
    "\n",
    "if COPY_UNCROPPED:\n",
    "    print(\"Copying uncropped images...\")\n",
    "    ABS_PATH_TRAIN_RAW = '/m2docs/res/data_raw/train'\n",
    "    c_count = [len(fnmatch.filter(os.listdir(os.path.join(ABS_PATH_TRAIN_RAW,c_name)), '*')) for c_name in classes]\n",
    "    print(classes)\n",
    "    print(c_count)\n",
    "\n",
    "    # add more images to largest class,\n",
    "    # and bring the others to the same count\n",
    "    print(\"Target count: n =\",target)\n",
    "\n",
    "    noise_modes = [None,'salt','pepper','s&p']\n",
    "    NOISE_AMOUNT = .01 # default amount\n",
    "    def add_noise(img, mode, noise_amount = .03):\n",
    "        if mode is not None:\n",
    "            gimg = skimage.util.random_noise(img, mode = mode, amount = random.uniform(0, noise_amount))\n",
    "            return gimg\n",
    "        else:\n",
    "            print(\"oops, you shouldn't see this\")\n",
    "\n",
    "    for c_name in classes:\n",
    "        current = uncropped_count[classes.index(c_name)]\n",
    "        if (verbose): \n",
    "            print(c_name, current, \"->\", target)\n",
    "        im_list = os.listdir(os.path.join(ABS_PATH_TRAIN_RAW, c_name))\n",
    "        while current < target:\n",
    "            filename = random.choice(im_list)\n",
    "            if (verbose): \n",
    "                print(filename)\n",
    "            out = Image.open(os.path.join(ABS_PATH_TRAIN_RAW, c_name, filename))\n",
    "\n",
    "            rot = random.randint(0,359)\n",
    "            mode = random.choice(noise_modes)\n",
    "            if (verbose): \n",
    "                print(filename,rot,mode,out.mode)\n",
    "            if (verbose): \n",
    "                print(filename.replace('.jpg','-x'+str(current)+'.jpg'))\n",
    "            if mode is not None:\n",
    "                Image.fromarray((add_noise(np.array(out.rotate(rot)),mode,NOISE_AMOUNT)*255).astype(np.uint8),pic.mode).save(os.path.join(ABS_PATH_TRAIN_RAW, c_name, filename.replace('.jpg','-x'+str(current)+'.jpg')))\n",
    "            else:\n",
    "                out.rotate(rot).save(os.path.join(ABS_PATH_TRAIN_RAW, c_name, filename.replace('.jpg','-x'+str(current)+'.jpg')))\n",
    "            current += 1\n",
    "\n",
    "    uncropped_count = []\n",
    "    for c in classes:\n",
    "        uncropped_count.append(len(fnmatch.filter(os.listdir(os.path.join(OUT_DIR_RAW + '/train/' + c + \"/\")), '*.jpg')))\n",
    "    print(uncropped_count)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os, datetime, math, io\n",
    "import tensorflow as tf\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp \n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature visualizer\n",
    "* [From Sovit Ranjan Rath](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)\n",
    "\n",
    "* Adapted to work with any model passed into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_features loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def savefeatures(model, writer, image = \"/m2docs/res/cropped_imgs/428-2.jpg\"):\n",
    "    if model is not None and writer is not None and image is not None:\n",
    "        # load the model\n",
    "        print(model)\n",
    "        model_cpu = model.cpu()\n",
    "        model_weights = [] # we will save the conv layer weights in this list\n",
    "        conv_layers = [] # we will save the 49 conv layers in this list\n",
    "        # get all the model children as list\n",
    "        model_children = list(model_cpu.children())\n",
    "\n",
    "        # counter to keep count of the conv layers\n",
    "        counter = 0 \n",
    "        # append all the conv layers and their respective weights to the list\n",
    "        for i in range(len(model_children)):\n",
    "            if type(model_children[i]) == nn.Conv2d:\n",
    "                counter += 1\n",
    "                model_weights.append(model_children[i].weight)\n",
    "                conv_layers.append(model_children[i])\n",
    "            elif type(model_children[i]) == nn.Sequential:\n",
    "                for j in range(len(model_children[i])):\n",
    "                    for child in model_children[i][j].children():\n",
    "                        if type(child) == nn.Conv2d:\n",
    "                            counter += 1\n",
    "                            model_weights.append(child.weight)\n",
    "                            conv_layers.append(child)\n",
    "        print(f\"Total convolutional layers: {counter}\")\n",
    "\n",
    "        # take a look at the conv layers and the respective weights\n",
    "        for weight, conv in zip(model_weights, conv_layers):\n",
    "            # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n",
    "            print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")\n",
    "\n",
    "        # visualize the first conv layer filters\n",
    "        plt.figure(figsize=(20, 17))\n",
    "        for i, filter in enumerate(model_weights[0]):\n",
    "            plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "            plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.savefig('outputs/filter.png')\n",
    "        plt.show()\n",
    "\n",
    "        # read and visualize an image\n",
    "        img = cv.imread(f\"{image}\")\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        # define the transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img = np.array(img)\n",
    "        # apply the transforms\n",
    "        img = transform(img)\n",
    "        print(img.size())\n",
    "        # unsqueeze to add a batch dimension\n",
    "        img = img.unsqueeze(0)\n",
    "        print(img.size())\n",
    "\n",
    "        # pass the image through all the layers\n",
    "        results = [conv_layers[0](img)]\n",
    "        for i in range(1, len(conv_layers)):\n",
    "            # pass the result from the last layer to the next layer\n",
    "            results.append(conv_layers[i](results[-1]))\n",
    "        # make a copy of the `results`\n",
    "        outputs = results\n",
    "\n",
    "        # visualize 64 features from each layer \n",
    "        # (although there are more feature maps in the upper layers)\n",
    "        for num_layer in range(len(outputs)):\n",
    "            figure = plt.figure(figsize=(30, 30))\n",
    "            layer_viz = outputs[num_layer][0, :, :, :]\n",
    "            layer_viz = layer_viz.data\n",
    "            print(layer_viz.size())\n",
    "            for i, filter in enumerate(layer_viz):\n",
    "                if i == 64: # we will visualize only 8x8 blocks from each layer\n",
    "                    break\n",
    "                plt.subplot(8, 8, i + 1)\n",
    "                plt.imshow(filter, cmap='gray')\n",
    "                plt.axis(\"off\")\n",
    "            print(f\"Saving layer {num_layer} feature maps...\")\n",
    "            plt.savefig(f\"outputs/layer_{num_layer}.png\")\n",
    "            writer.add_figure(\"features/layer_\"+str(num_layer), figure)\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "print(\"save_features loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier\n",
    "Based on [this](https://nextjournal.com/gkoehler/pytorch-mnist) pytorch tutorial\n",
    "\n",
    "Load images and train classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning.\n",
      "Channels: 3 options: 64 to 256\n",
      "Dropout: 3 options: 0.1, 0.3, 0.5\n",
      "Optimizer: 3 options, including 'sgd'\n",
      "Cropped: 3 options (True/False)\n",
      "Network: Net1 or Net2\n",
      "-> Starting trial run-0\n",
      "{'channels': 64, 'dropout': 0.1, 'optimizer': 'sgd', 'processed (yolo)': False, 'network': 'Net1'}\n",
      "Running using uncropped (plain) images\n",
      "Device: cuda:0\n",
      " Loading network 1\n",
      "Net1(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 8, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv4): Conv2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv5): Conv2d(16, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=57600, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "  Epoch: 0 [24/128/35930 (0%)]\tLoss: 1.609976\tAcc: 18.750000%\n",
      "  Epoch: 0 [2551/12928/35930 (36%)]\tLoss: 1.609405\tAcc: 19.732364%\n",
      "  Epoch: 0 [5096/25728/35930 (71%)]\tLoss: 1.610224\tAcc: 19.807214%\n",
      "Epoch: 0\tLoss: 1.609510\tAcc: 19.844141\n",
      " Validation correct: 255.0 / 1280\n",
      "Validation: acc: 19.921875%\tloss: 0.012574\n",
      " Testing correct: 128.0 / 640\n",
      "Testing: acc: 20.000000%\tloss: -0.039566\n",
      "Best: 0 @ 19.921875% -> epoch target 15\n",
      "  Epoch: 1 [32/128/35930 (0%)]\tLoss: 1.608373\tAcc: 25.000000%\n",
      "  Epoch: 1 [2643/12928/35930 (36%)]\tLoss: 1.609798\tAcc: 20.443998%\n",
      "  Epoch: 1 [5164/25728/35930 (71%)]\tLoss: 1.609256\tAcc: 20.071517%\n",
      "Epoch: 1\tLoss: 1.609488\tAcc: 19.869190\n",
      " Validation correct: 262.0 / 1280\n",
      "Validation: acc: 20.468750%\tloss: 0.012574\n",
      " Testing correct: 128.0 / 640\n",
      "Testing: acc: 20.000000%\tloss: -0.032315\n",
      "Best: 1 @ 20.468750% -> epoch target 15\n",
      "  Epoch: 2 [29/128/35930 (0%)]\tLoss: 1.609282\tAcc: 22.656250%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-114aad6fe74b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m                             \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhpdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                             \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhpdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_false'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                         \u001b[0msession_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-114aad6fe74b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_dir, hparams)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running using uncropped (plain) images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_stretch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETRIC_ACCURACY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-114aad6fe74b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(max_epochs, min_epochs, epoch_stretch, batch_size, train_path, valid_path, test_path, labels, hparams, writer)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mcategorical_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "length = 256 #256\n",
    "skip_all = False\n",
    "\n",
    "classes = ['Apis_mellifera','Bombus_impatiens','Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n",
    "#classes = ['Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n",
    "\n",
    "ABS_PATH_TRAIN = '/m2docs/res/data/train'\n",
    "ABS_PATH_VALID = '/m2docs/res/data/valid'\n",
    "ABS_PATH_TEST = '/m2docs/res/data/test'\n",
    "ABS_PATH_TRAIN_RAW = '/m2docs/res/data_raw/train'\n",
    "ABS_PATH_VALID_RAW = '/m2docs/res/data_raw/valid'\n",
    "ABS_PATH_TEST_RAW = '/m2docs/res/data_raw/test'\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def load(dir_name, batch_size, shuffle = False):\n",
    "    return(\n",
    "        #create a data loader\n",
    "        torch.utils.data.DataLoader(\n",
    "            datasets.ImageFolder(root = dir_name, \n",
    "                                 transform = transforms.Compose([\n",
    "                                     transforms.Resize((length,length)),\n",
    "                                     transforms.ToTensor()\n",
    "                                 ])),\n",
    "            batch_size = batch_size,\n",
    "            num_workers = 8,\n",
    "            shuffle = shuffle,\n",
    "        )\n",
    "    )\n",
    "\n",
    "class Net1(nn.Module):\n",
    "#     def __init__(self, hparams):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.dropout = nn.Dropout2d(p = hparams[HP_DROPOUT])\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(3, hparams[HP_NUM_UNITS]//32, 2, padding = 2)\n",
    "#         self.conv2 = nn.Conv2d(hparams[HP_NUM_UNITS]//32, hparams[HP_NUM_UNITS]//32, 5, padding = 4) \n",
    "#         self.conv3 = nn.Conv2d(hparams[HP_NUM_UNITS]//32, hparams[HP_NUM_UNITS]//16, 2, padding = 1)\n",
    "#         self.conv4 = nn.Conv2d(hparams[HP_NUM_UNITS]//16, hparams[HP_NUM_UNITS]//8, 5, padding = 4)\n",
    "#         self.conv5 = nn.Conv2d(hparams[HP_NUM_UNITS]//8, hparams[HP_NUM_UNITS], 3) \n",
    "                \n",
    "#         self.fc1 = nn.Linear(hparams[HP_NUM_UNITS]*34**2, (hparams[HP_NUM_UNITS]*34**2)//2)\n",
    "#         self.fc2 = nn.Linear((hparams[HP_NUM_UNITS]*34**2)//2, length)\n",
    "#         self.fc3 = nn.Linear(length, len(classes))\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(Net1, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout2d(p = hparams[HP_DROPOUT])\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, hparams[HP_NUM_UNITS]//8, 2)\n",
    "        self.conv2 = nn.Conv2d(hparams[HP_NUM_UNITS]//8, hparams[HP_NUM_UNITS]//8, 2) \n",
    "        self.conv3 = nn.Conv2d(hparams[HP_NUM_UNITS]//8, hparams[HP_NUM_UNITS]//4, 2)\n",
    "        self.conv4 = nn.Conv2d(hparams[HP_NUM_UNITS]//4, hparams[HP_NUM_UNITS]//4, 2)\n",
    "        self.conv5 = nn.Conv2d(hparams[HP_NUM_UNITS]//4, hparams[HP_NUM_UNITS], 2) \n",
    "                \n",
    "        self.fc1 = nn.Linear(hparams[HP_NUM_UNITS]*30**2, length*4)\n",
    "        self.fc2 = nn.Linear(length*4, length//2)\n",
    "        self.fc3 = nn.Linear(length//2, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(Net2, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout2d(p = hparams[HP_DROPOUT])\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, hparams[HP_NUM_UNITS]//4, 2)\n",
    "        self.conv2 = nn.Conv2d(hparams[HP_NUM_UNITS]//4, hparams[HP_NUM_UNITS]//2, 2) \n",
    "        self.conv3 = nn.Conv2d(hparams[HP_NUM_UNITS]//2, hparams[HP_NUM_UNITS], 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hparams[HP_NUM_UNITS]*31**2, length)\n",
    "        self.fc2 = nn.Linear(length, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def validate(network,device,load_valid,optimizer,criterion = nn.CrossEntropyLoss()):\n",
    "    network.train().to(device)\n",
    "    correct = 0\n",
    "    valid_loss = 0\n",
    "    for index, data in enumerate(load_valid, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        # gather accuracy stats:\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).float().sum().item()\n",
    "    print(\" Validation correct: {} / {}\".format(correct,len(load_valid.dataset)))  \n",
    "    accuracy = 100 * correct / len(load_valid.dataset)\n",
    "    valid_loss = valid_loss / len(load_valid.dataset)\n",
    "    return accuracy, valid_loss\n",
    "\n",
    "def test(network,device,load_test,criterion = nn.CrossEntropyLoss()):\n",
    "    network.eval().to(device)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(load_test, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = network(inputs).to(device)\n",
    "            test_loss += nn.functional.nll_loss(outputs, labels).item()\n",
    "\n",
    "            # gather accuracy stats:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "    print(\" Testing correct: {} / {}\".format(correct,len(load_test.dataset)))        \n",
    "    accuracy = 100 * correct / len(load_test.dataset)\n",
    "    valid_loss = test_loss / len(load_test.dataset)\n",
    "    return (accuracy, test_loss)\n",
    "\n",
    "def train(max_epochs, min_epochs, epoch_stretch, batch_size, train_path, valid_path, test_path, labels, hparams, writer):\n",
    "    epochs = max_epochs\n",
    "    class_names = labels\n",
    "    num_classes = len(class_names)\n",
    "    train_batch = batch_size\n",
    "    test_batch = batch_size\n",
    "    SAVE_PATH = '/m2docs/res/trained_models/model'\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device: {}\".format(device))\n",
    "    if (hparams[HP_NETWORK] == \"Net1\"):\n",
    "        print(\" Loading network 1\")\n",
    "        net = Net1(hparams).to(device)\n",
    "    else:\n",
    "        print(\" Loading network 2\")\n",
    "        net = Net2(hparams).to(device)\n",
    "    print(net)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    tag = datetime.datetime.now().strftime(\".%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    VISUAL_OUT = '/m2docs/res/visualizations/' + tag + \"/\"\n",
    "    if not os.path.exists(VISUAL_OUT):\n",
    "        os.makedirs(VISUAL_OUT)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Choose optimizer: (from ['adam','sgd','adagrad'])\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    if (hparams[HP_OPTIMIZER] == 'adam'):\n",
    "        optimizer = optim.Adam(net.parameters(), lr = .001)\n",
    "    elif (hparams[HP_OPTIMIZER] == 'sgd'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr = .001, momentum=0.1)\n",
    "    elif (hparams[HP_OPTIMIZER] == 'adagrad'):\n",
    "        optimizer = optim.Adagrad(net.parameters(), lr = .001)\n",
    "    else:\n",
    "        # default\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "    torch.manual_seed(417)\n",
    "    \n",
    "    load_train = load(train_path, batch_size, shuffle=True)\n",
    "    load_valid = load(valid_path, batch_size, shuffle=True)\n",
    "    load_test  = load(test_path, batch_size, shuffle=True)\n",
    "\n",
    "    validation_accuracies = []\n",
    "    best_epoch = 0\n",
    "    epoch = 0\n",
    "    killed = False\n",
    "    print(type(list(net.children())[2].weight))\n",
    "    while (epoch <= best_epoch + epoch_stretch or epoch < min_epochs) and epoch < max_epochs and not killed:\n",
    "        net.train()\n",
    "        run_loss = 0.0\n",
    "        sum_loss = 0.0\n",
    "        count = 0\n",
    "        correct = 0.0\n",
    "        categorical_correct = [0.0 for i in range(num_classes)]\n",
    "        for index, data in enumerate(load_train, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # gather accuracy stats:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "            for i in range(num_classes):\n",
    "                categorical_correct[i] += ((predicted==i) == (labels==i)).float().sum().item()\n",
    "                #print(classes[i],categorical_correct[i]/((index+1)*batch_size))\n",
    "            \n",
    "            # print statistics\n",
    "            run_loss += loss.item()\n",
    "            sum_loss += loss.item()\n",
    "            count += 1 \n",
    "            if index % 100 == 0:    # print every 100 mini-batches\n",
    "                print('  Epoch: {} [{}/{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}%'.format(\n",
    "                    epoch, int(correct), (index + 1) * batch_size, len(load_train.dataset),\n",
    "                    100. * index / len(load_train), loss.item(), 100. * correct / ((index+1) * batch_size)))\n",
    "                run_loss = 0.0\n",
    "        accuracy = 100. * correct / len(load_train.dataset)\n",
    "        print('Epoch: {}\\tLoss: {:.6f}\\tAcc: {:.6f}'.format(\n",
    "                epoch, sum_loss/count, accuracy))\n",
    "        writer.add_scalar(\"Loss/train\", sum_loss/count, epoch)\n",
    "        writer.add_scalar(\"Acc/train\", accuracy, epoch)\n",
    "        \n",
    "        modules_list = iter(net.named_modules())\n",
    "        next(modules_list)\n",
    "        for module in modules_list:\n",
    "            try:\n",
    "                writer.add_histogram(\"Model/\"+module[0]+\".weights\", module[1].weight, epoch)\n",
    "                writer.add_histogram(\"Model/\"+module[0]+\".bias\", module[1].bias, epoch)\n",
    "            except:\n",
    "                pass\n",
    "        # Categorical accuracy:\n",
    "        for i in range(num_classes):\n",
    "            writer.add_scalar(\"Acc/\" + classes[i],categorical_correct[i]/len(load_train.dataset), epoch)\n",
    "        torch.save(net.state_dict(), SAVE_PATH + tag + \"-progress\")\n",
    "        \n",
    "        # get validation accuracy: \n",
    "        valid_acc, valid_loss = validate(net, device, load_valid, optimizer, criterion)\n",
    "        writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/valid\", valid_acc, epoch)\n",
    "        print('Validation: acc: {:.6f}%\\tloss: {:.6f}'.format(\n",
    "                valid_acc, valid_loss))\n",
    "        validation_accuracies.append(valid_acc)\n",
    "        best_epoch = validation_accuracies.index(max(validation_accuracies))\n",
    "        \n",
    "        # get test accuracy: \n",
    "        test_acc, test_loss = test(net, device, load_test, criterion)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/test\", test_acc, epoch)\n",
    "        print('Testing: acc: {:.6f}%\\tloss: {:.6f}'.format(\n",
    "                test_acc, test_loss))\n",
    "        \n",
    "        # this is the best epoch so far, save these weights:\n",
    "        if (best_epoch == epoch):\n",
    "            torch.save(net.state_dict(), SAVE_PATH + tag + \"-best\")\n",
    "        if (epoch >= 2):\n",
    "            if (validation_accuracies[epoch] == 100/num_classes):\n",
    "                killed = True\n",
    "                print(\"[!] This run has failed, accuracies are bad. Aborting.\")\n",
    "        \n",
    "        print('Best: {} @ {:.6f}% -> epoch target {}'.format(best_epoch,validation_accuracies[best_epoch],max([best_epoch+epoch_stretch,min_epochs])))\n",
    "        epoch += 1\n",
    "        \n",
    "    print('Done training.')\n",
    "    torch.save(net.state_dict(), SAVE_PATH + tag + \"-final\")\n",
    "    \n",
    "    ## TODO: test here\n",
    "\n",
    "    prediction_list = torch.zeros(0,dtype=torch.long).to(device)\n",
    "    label_list = torch.zeros(0,dtype=torch.long).to(device)\n",
    "    \n",
    "    ## Testing\n",
    "    net.eval().to(device)\n",
    "    correct = 0\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(load_test, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            valid_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            prediction_list = torch.cat([prediction_list, predicted.view(-1)])  \n",
    "            label_list = torch.cat([label_list, labels.view(-1)])\n",
    "            #print(\"Predictions, ground:\")\n",
    "            #print(prediction_list)\n",
    "            #print(label_list)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "    print(\" Testing correct: {} / {}\".format(correct,len(load_test.dataset)))        \n",
    "    t_acc = 100 * correct / len(load_test.dataset)\n",
    "    t_loss = valid_loss / len(load_test.dataset)\n",
    "\n",
    "    \n",
    "    matrix = confusion_matrix(label_list.cpu().numpy(), prediction_list.cpu().numpy())\n",
    "    print(matrix)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(matrix)\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(classes)))\n",
    "    ax.set_yticks(np.arange(len(classes)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticklabels(classes)\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            text = ax.text(j, i, \"{:4.2f}\".format(matrix[i, j]/sum(matrix[i])),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Testing confusion matrix (n = {})\".format(len(load_test.dataset)))\n",
    "\n",
    "    plt.show()\n",
    "    writer.add_figure('Testing/conf',fig)\n",
    "    \n",
    "    if (hparams[HP_YOLOCROPPED]):\n",
    "        # cropped images\n",
    "        savefeatures(net, writer, image = \"/m2docs/res/data/test/Apis_mellifera/2001-1.jpg\")\n",
    "    else:\n",
    "        savefeatures(net, writer, image = \"/m2docs/res/data_raw/test/Apis_mellifera/2001-1.jpg\")\n",
    "    \n",
    "    ## Visualise features\n",
    "#     FV = FilterVisualizer(net,VISUAL_OUT)\n",
    "#     image_out = reconstructions_single_layer(list((net.children()))[2],'Layer 1 Block 1 Conv1',\n",
    "#                                              list(range(6,12)),n_cols=3,\n",
    "#                                              save_fig=True,album_hash=None)\n",
    "    \n",
    "    class_accuracy=100*matrix.diagonal() / matrix.sum(1)\n",
    "    #print(classes)\n",
    "    #print(class_accuracy)\n",
    "    for i in range(len(classes)):\n",
    "        print(\"{}: {:.4f}\".format(classes[i],class_accuracy[i]))\n",
    "    print(\"Best val_acc: {:6.4f}\".format(max(validation_accuracies)))\n",
    "    return max(validation_accuracies)\n",
    "\n",
    "# magic here.\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        run_writer = SummaryWriter(log_dir = run_dir)            \n",
    "        if (hparams[HP_YOLOCROPPED]):\n",
    "            train_path = ABS_PATH_TRAIN\n",
    "            valid_path = ABS_PATH_VALID\n",
    "            test_path = ABS_PATH_TEST\n",
    "            print(\"Running using cropped (yolo) images\")\n",
    "        else:\n",
    "            train_path = ABS_PATH_TRAIN_RAW\n",
    "            valid_path = ABS_PATH_VALID_RAW\n",
    "            test_path = ABS_PATH_TEST_RAW\n",
    "            print(\"Running using uncropped (plain) images\")\n",
    "        \n",
    "        accuracy = train(max_epochs = 100, min_epochs = 15, epoch_stretch = 5, batch_size = 128, train_path = train_path, valid_path = valid_path, test_path = test_path, labels = classes, hparams = hparams, writer = run_writer)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "if (skip_all is False): \n",
    "    HP_NUM_UNITS = hp.HParam('channels', hp.Discrete([64, 128, 256]))\n",
    "    HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.5))\n",
    "    #HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['sgd','adagrad']))\n",
    "    HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['sgd']))\n",
    "    HP_YOLOCROPPED = hp.HParam('processed (yolo)', hp.Discrete([True, False]))\n",
    "    HP_NETWORK = hp.HParam('network', hp.Discrete([\"Net2\",\"Net1\"]))\n",
    "    METRIC_ACCURACY = 'accuracy'\n",
    "    \n",
    "    hpdirname = 'runs/' + (datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + ':hparam_tuning')\n",
    "    with tf.summary.create_file_writer(hpdirname).as_default():\n",
    "        hp.hparams_config(\n",
    "            hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_YOLOCROPPED],\n",
    "            metrics=[hp.Metric(METRIC_ACCURACY, display_name='best accuracy (validation)')]\n",
    "        )\n",
    "    \n",
    "    session_num = 0\n",
    "    print(\"Hyperparameter tuning.\")\n",
    "    print(\"Channels: {} options: {} to {}\".format(len(HP_NUM_UNITS.domain.values), HP_NUM_UNITS.domain.values[0], HP_NUM_UNITS.domain.values[len( HP_NUM_UNITS.domain.values)-1]))\n",
    "    print(\"Dropout: {} options: {}, {}, {}\".format(3, HP_DROPOUT.domain.min_value, ((HP_DROPOUT.domain.min_value + HP_DROPOUT.domain.max_value)/2), HP_DROPOUT.domain.max_value))\n",
    "    print(\"Optimizer: {} options, including '{}'\".format(len(HP_NUM_UNITS.domain.values), HP_OPTIMIZER.domain.values[0]))\n",
    "    print(\"Cropped: {} options (True/False)\".format(len(HP_NUM_UNITS.domain.values)))\n",
    "    print(\"Network: {} or {}\".format(HP_NETWORK.domain.values[0],HP_NETWORK.domain.values[1]))\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for dropout_rate in (HP_DROPOUT.domain.min_value, ((HP_DROPOUT.domain.min_value + HP_DROPOUT.domain.max_value)/2), HP_DROPOUT.domain.max_value):\n",
    "            for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                for network in HP_NETWORK.domain.values:\n",
    "                    for yolo in HP_YOLOCROPPED.domain.values:\n",
    "                        hparams = {\n",
    "                            HP_NUM_UNITS: num_units,\n",
    "                            HP_DROPOUT: float(\"%0.2f\"%float(dropout_rate)),\n",
    "                            HP_OPTIMIZER: optimizer,\n",
    "                            HP_YOLOCROPPED: yolo,\n",
    "                            HP_NETWORK: network,\n",
    "                        }\n",
    "                        torch.cuda.empty_cache()\n",
    "                        run_name = \"run-%d\" % session_num\n",
    "                        print(\"-> Starting trial %s\" % (run_name))\n",
    "                        print({h.name: hparams[h] for h in hparams})\n",
    "                        if hparams[HP_YOLOCROPPED]:\n",
    "                            run(os.path.join(hpdirname, '_' + str(session_num) + '_true'), hparams)\n",
    "                        else:\n",
    "                            run(os.path.join(hpdirname, '_' + str(session_num)) + '_false', hparams)\n",
    "                        session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 stage classifier\n",
    "\n",
    "1. Train to detect *Apis Mellifera* vs. *Bombus Auricomus* vs. a merged set of the other classes.\n",
    "2. Train a second model to tell apart the other three classes.'\n",
    "3. Note that both of these datasets will by default be unbalanced unless something is changed above.\n",
    "4. When running detections, if the first model's confidence is below a threshold, apply the second model to see if there is an improved prediction.\n",
    "\n",
    "`todo: balance datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os, datetime, math\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "length = 512\n",
    "var_droupout = 0.2\n",
    "skip_all = True\n",
    "tag1 = ':' + str(length) + '_STAGE-1_' + str(var_droupout)\n",
    "tag2 = ':' + str(length) + '_STAGE-2_' + str(var_droupout)\n",
    "writer1 = SummaryWriter(log_dir = os.path.join('runs/',(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + tag1)))\n",
    "writer2 = SummaryWriter(log_dir = os.path.join('runs/',(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + tag2)))\n",
    "print(\"tensorboard writing\"+tag1)\n",
    "print(\"tensorboard writing\"+tag2)\n",
    "\n",
    "classes1 = ['Apis_mellifera','Bombus_impatiens','Merged']\n",
    "classes2 = ['Bombus_auricomus','Bombus_bimaculatus','Bombus_griseocollis']\n",
    "\n",
    "ABS_PATH_TRAIN1 = '/m2docs/res/data1/train'\n",
    "ABS_PATH_VALID1 = '/m2docs/res/data1/valid'\n",
    "ABS_PATH_TEST1 = '/m2docs/res/data1/test'\n",
    "ABS_PATH_TRAIN2 = '/m2docs/res/data2/train'\n",
    "ABS_PATH_VALID2 = '/m2docs/res/data2/valid'\n",
    "ABS_PATH_TEST2 = '/m2docs/res/data2/test'\n",
    "\n",
    "class SaveFeatures():\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "class FilterVisualizer():\n",
    "        def __init__(self, network, OUTPUT_DIR):\n",
    "                self.model = nn.Sequential(*list(network.children())[:-2]).cuda().eval()\n",
    "                self.network = network\n",
    "                self.OUTPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "        def visualize(self, sz, layer, filter, upscaling_steps=12, upscaling_factor=1.2, lr=0.1, opt_steps=20, blur=None, save=False, print_losses=False):\n",
    "                with Torch.no_grad():\n",
    "                    img = (np.random.random((sz, sz, 3)) * 20 + 128.)/255.\n",
    "    #                img = np.random.uniform(0, 1, size=(sz, sz, 3)).astype(np.float32)\n",
    "    #                median_filter_size = 4 if sz < 100 else 8\n",
    "    #                img = scipy.ndimage.filters.median_filter(img, [median_filter_size,median_filter_size,1])\n",
    "\n",
    "                    activations = SaveFeatures(layer)  # register hook\n",
    "\n",
    "                    for i in range(upscaling_steps):  # scale the image up upscaling_steps times\n",
    "                            train_tfms, val_tfms = tfms_from_model(network, sz)\n",
    "                            img_var = V(val_tfms(img)[None], requires_grad=True)  # convert image to Variable that requires grad\n",
    "                            optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n",
    "                            if i > upscaling_steps/2:\n",
    "                                    opt_steps_ = int(opt_steps*1.3)\n",
    "                            else:\n",
    "                                    opt_steps_ = opt_steps\n",
    "                            for n in range(opt_steps_):  # optimize pixel values for opt_steps times\n",
    "                                    optimizer.zero_grad()\n",
    "                                    self.model(img_var)\n",
    "                                    loss = -1 * activations.features[0, filter].mean()\n",
    "                                    if print_losses:\n",
    "                                            if i%3==0 and n%5==0:\n",
    "                                                    print(f'{i} - {n} - {float(loss)}')\n",
    "                                    loss.backward()\n",
    "                                    optimizer.step()\n",
    "                            img = val_tfms.denorm(np.rollaxis(to_np(img_var.data),1,4))[0]\n",
    "                            self.output = img\n",
    "                            sz = int(upscaling_factor * sz)  # calculate new image size\n",
    "                            img = cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC)  # scale image up\n",
    "                            if blur is not None: img = cv2.blur(img,(blur,blur))  # blur image to reduce high frequency patterns\n",
    "                    activations.close()\n",
    "                    return np.clip(self.output, 0, 1)\n",
    "        \n",
    "        def get_transformed_img(self,img,sz):\n",
    "            with Torch.no_grad():\n",
    "                train_tfms, val_tfms = tfms_from_model(network, sz)\n",
    "                return val_tfms.denorm(np.rollaxis(to_np(val_tfms(img)[None]),1,4))[0]\n",
    "        \n",
    "        def most_activated(self, image, layer, limit_top=None):\n",
    "            with Torch.no_grad():\n",
    "                train_tfms, val_tfms = tfms_from_model(network, 224)\n",
    "                transformed = val_tfms(image)\n",
    "\n",
    "                activations = SaveFeatures(layer)  # register hook\n",
    "                self.model(V(transformed)[None]);\n",
    "                \n",
    "                mean_act = [activations.features[0,i].mean().data.cpu().numpy()[0] for i in range(activations.features.shape[1])]\n",
    "                activations.close()\n",
    "                return mean_act\n",
    "\n",
    "def plot_reconstructions_single_layer(imgs,layer_name,filters,\n",
    "                                      n_cols=3,\n",
    "                                      cell_size=4,save_fig=True,\n",
    "                                      album_hash=None):\n",
    "        n_rows = ceil((len(imgs))/n_cols)\n",
    "\n",
    "        fig,axes = plt.subplots(n_rows,n_cols, figsize=(cell_size*n_cols,cell_size*n_rows))\n",
    "                    \n",
    "        for i,ax in enumerate(axes.flat):\n",
    "                ax.grid(False)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "\n",
    "                if i>=len(filters):\n",
    "                        pass\n",
    "\n",
    "                ax.set_title(f'fmap {filters[i]}')\n",
    "\n",
    "                ax.imshow(imgs[i])\n",
    "        fig.suptitle(f'cnn {layer_name}', fontsize=\"x-large\",y=1.0)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.88)\n",
    "        save_name = layer_name.lower().replace(' ','_')\n",
    "        if save_fig:\n",
    "                plt.savefig(f'' + OUTPUT_DIR + 'network_{save_name}_fmaps_{\"_\".join([str(f) for f in filters])}.png')\n",
    "                plt.close()\n",
    "                return True\n",
    "        else:\n",
    "                plt.show()\n",
    "                return None\n",
    "\n",
    "def reconstructions_single_layer(layer,layer_name,filters,\n",
    "                                 init_size=56, upscaling_steps=12, \n",
    "                                 upscaling_factor=1.2, \n",
    "                                 opt_steps=20, blur=5,\n",
    "                                 lr=1e-1,print_losses=False,\n",
    "                                 n_cols=3, cell_size=4,\n",
    "                                 save_fig=True,album_hash=None):\n",
    "        \n",
    "        imgs = []\n",
    "        for i in range(len(filters)):\n",
    "                imgs.append(FV.visualize(init_size,layer, filters[i], \n",
    "                            upscaling_steps=upscaling_steps, \n",
    "                            upscaling_factor=upscaling_factor, \n",
    "                            opt_steps=opt_steps, blur=blur,\n",
    "                            lr=lr,print_losses=print_losses))\n",
    "                \n",
    "        return plot_reconstructions_single_layer(imgs,layer_name,filters,\n",
    "                                                 n_cols=n_cols,cell_size=cell_size,\n",
    "                                                 save_fig=save_fig,album_hash=album_hash)\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def load(dir_name, batch_size, shuffle = False):\n",
    "    return(\n",
    "        #create a data loader\n",
    "        torch.utils.data.DataLoader(\n",
    "            datasets.ImageFolder(root = dir_name, transform = transforms.ToTensor()),\n",
    "            batch_size = batch_size,\n",
    "            num_workers = 2,\n",
    "            shuffle = shuffle\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Layer/network 1:\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout2d(p = var_droupout)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*28*28, length)\n",
    "        self.fc2 = nn.Linear(length, int(length/2))\n",
    "        #self.fc3 = nn.Linear(int(length/2), int(length/4))\n",
    "        self.fc3 = nn.Linear(int(length/2), 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv2(x))))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv3(x))))\n",
    "        x = (F.relu(self.pool(self.conv4(x))))\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "# Layer/network 2:    \n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout2d(p = var_droupout)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*28*28, length)\n",
    "        self.fc2 = nn.Linear(length, int(length/2))\n",
    "        #self.fc3 = nn.Linear(int(length/2), int(length/4))\n",
    "        self.fc3 = nn.Linear(int(length/2), 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv2(x))))\n",
    "        x = self.dropout(F.relu(self.pool(self.conv3(x))))\n",
    "        x = (F.relu(self.pool(self.conv4(x))))\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "def validate(network,device,load_valid,criterion = nn.CrossEntropyLoss()):\n",
    "    network.eval().to(device)\n",
    "    correct = 0\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(load_valid, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = network(inputs)\n",
    "            valid_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # gather accuracy stats:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "    accuracy = 100 * correct / len(load_valid.dataset)\n",
    "    valid_loss = valid_loss / len(load_valid.dataset)\n",
    "    return accuracy , valid_loss\n",
    "\n",
    "def train(max_epochs = 50, min_epochs = 5, epoch_stretch = 5, train_path = ABS_PATH_TRAIN, valid_path = ABS_PATH_VALID, test_path = ABS_PATH_TEST, labels = classes, batch_size = 24):\n",
    "    epochs = max_epochs\n",
    "    class_names = labels\n",
    "    num_classes = len(class_names)\n",
    "    train_batch = batch_size\n",
    "    test_batch = 128\n",
    "    SAVE_PATH = '/m2docs/res/models'\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device: {}\".format(device))\n",
    "    net = Net().to(device)\n",
    "    print(net)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "    torch.manual_seed(417)\n",
    "    \n",
    "    load_train = load(train_path, batch_size, shuffle=True)\n",
    "    load_valid = load(valid_path, batch_size, shuffle=True)\n",
    "    load_test  = load(test_path, batch_size, shuffle=True)\n",
    "\n",
    "    validation_accuracies = []\n",
    "    best_epoch = 0\n",
    "    epoch = 0\n",
    "    while (epoch <= best_epoch + epoch_stretch or epoch < min_epochs) and epoch < max_epochs:\n",
    "        net.train()\n",
    "        run_loss = 0.0\n",
    "        sum_loss = 0.0\n",
    "        count = 0\n",
    "        correct = 0.0\n",
    "        categorical_correct = [0.0 for i in range(num_classes)]\n",
    "        for index, data in enumerate(load_train, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # gather accuracy stats:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).float().sum().item()\n",
    "            for i in range(num_classes):\n",
    "                categorical_correct[i] += ((predicted==i) == (labels==i)).float().sum().item()\n",
    "                #print(classes[i],categorical_correct[i]/((index+1)*batch_size))\n",
    "            \n",
    "            # print statistics\n",
    "            run_loss += loss.item()\n",
    "            sum_loss += loss.item()\n",
    "            count += 1 \n",
    "            if index % 200 == 0:    # print every 200 mini-batches\n",
    "                print('  Epoch: {} [{}/{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}%'.format(\n",
    "                    epoch, int(correct), (index + 1) * batch_size, len(load_train.dataset),\n",
    "                    100. * index / len(load_train), loss.item(), 100. * correct / ((index+1) * batch_size)))\n",
    "                run_loss = 0.0\n",
    "        accuracy = 100. * correct / len(load_train.dataset)\n",
    "        print('Epoch: {}\\tLoss: {:.6f}\\tAcc: {:.6f}'.format(\n",
    "                epoch, sum_loss/count, accuracy))\n",
    "        writer.add_scalar(\"Loss/train\", sum_loss/count, epoch)\n",
    "        writer.add_scalar(\"Acc/train\", accuracy, epoch)\n",
    "        \n",
    "        modules_list = iter(net.named_modules())\n",
    "        next(modules_list)\n",
    "        for module in modules_list:\n",
    "            try:\n",
    "                writer.add_histogram(\"Model/\"+module[0]+\".weights\", module[1].weight, epoch)\n",
    "                writer.add_histogram(\"Model/\"+module[0]+\".bias\", module[1].bias, epoch)\n",
    "            except:\n",
    "                pass\n",
    "        # Categorical accuracy:\n",
    "        for i in range(num_classes):\n",
    "            writer.add_scalar(\"Acc/\" + classes[i],categorical_correct[i]/len(load_train.dataset), epoch)\n",
    "        torch.save(net.state_dict(), SAVE_PATH+\"_progress\")\n",
    "        \n",
    "        # get validation accuracy: \n",
    "        valid_acc, valid_loss = validate(net, device, load_valid, criterion)\n",
    "        writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/valid\", valid_acc, epoch)\n",
    "        print('Validation: acc: {:.6f}%\\tloss: {:.6f}'.format(\n",
    "                valid_acc, valid_loss))\n",
    "        validation_accuracies.append(valid_acc)\n",
    "        best_epoch = validation_accuracies.index(max(validation_accuracies))\n",
    "        print('Best: {} @ {:.6f}% -> epoch target {}'.format(best_epoch,validation_accuracies[best_epoch],max([best_epoch+epoch_stretch,min_epochs])))\n",
    "        epoch += 1\n",
    "        \n",
    "    print('Done training.')\n",
    "    torch.save(net.state_dict(), SAVE_PATH)\n",
    "\n",
    "    prediction_list = torch.zeros(0,dtype=torch.long).to(device)\n",
    "    label_list = torch.zeros(0,dtype=torch.long).to(device)\n",
    "              \n",
    "    with torch.no_grad():\n",
    "        for data in load_test:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "              \n",
    "            prediction_list = torch.cat([prediction_list, predicted.view(-1)])  \n",
    "            label_list = torch.cat([label_list, labels.view(-1)])\n",
    "    \n",
    "    matrix = confusion_matrix(label_list.cpu().numpy(), prediction_list.cpu().numpy())\n",
    "    print(matrix)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(matrix)\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(classes)))\n",
    "    ax.set_yticks(np.arange(len(classes)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticklabels(classes)\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            text = ax.text(j, i, matrix[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Testing confusion matrix\")\n",
    "\n",
    "    plt.show()\n",
    "    writer.add_figure('Testing/conf',fig)\n",
    "    \n",
    "    class_accuracy=100*matrix.diagonal() / matrix.sum(1)\n",
    "    print(classes)\n",
    "    print(class_accuracy)\n",
    "\n",
    "if (skip_all is False):\n",
    "    # setup: create 2-stage directories:\n",
    "    # data1: first stage: mellifera, impatiens, 3-merged\n",
    "    %mkdir data1\n",
    "    %rm -r data1/*\n",
    "    %mkdir data1/train\n",
    "    %cp -r data/train/Apis_mellifera data1/train/Apis_mellifera\n",
    "    %cp -r data/train/Bombus_auricomus data1/train/Bombus_auricomus\n",
    "    %cp -r data/train/Bombus_bimaculatus data1/train/Merged\n",
    "    %cp data/train/Bombus_griseocollis/* data1/train/Merged\n",
    "    %cp data/train/Bombus_impatiens/* data1/train/Merged\n",
    "    %mkdir data1/valid\n",
    "    %cp -r data/valid/Apis_mellifera data1/valid/Apis_mellifera\n",
    "    %cp -r data/valid/Bombus_auricomus data1/valid/Bombus_auricomus\n",
    "    %cp -r data/valid/Bombus_bimaculatus data1/valid/Merged\n",
    "    %cp data/valid/Bombus_griseocollis/* data1/valid/Merged\n",
    "    %cp data/valid/Bombus_impatiens/* data1/valid/Merged\n",
    "    %mkdir data1/test\n",
    "    %cp -r data/test/Apis_mellifera data1/test/Apis_mellifera\n",
    "    %cp -r data/test/Bombus_auricomus data1/test/Bombus_auricomus\n",
    "    %cp -r data/test/Bombus_bimaculatus data1/test/Merged\n",
    "    %cp data/test/Bombus_griseocollis/* data1/test/Merged\n",
    "    %cp data/test/Bombus_impatiens/* data1/test/Merged\n",
    "\n",
    "    # data2: auricomus vs bimaculatus vs griseocollis\n",
    "    %mkdir data2\n",
    "    %rm -r data2/*\n",
    "    %mkdir data2/train\n",
    "    %cp -r data/train/Bombus_bimaculatus data2/train/Bombus_bimaculatus\n",
    "    %cp -r data/train/Bombus_griseocollis data2/train/Bombus_griseocollis\n",
    "    %cp -r data/train/Bombus_impatiens data2/train/Bombus_impatiens\n",
    "    %mkdir data2/valid\n",
    "    %cp -r data/valid/Bombus_bimaculatus data2/valid/Bombus_bimaculatus\n",
    "    %cp -r data/valid/Bombus_griseocollis data2/valid/Bombus_griseocollis\n",
    "    %cp -r data/valid/Bombus_impatiens data2/valid/Bombus_impatiens\n",
    "    %mkdir data2/test\n",
    "    %cp -r data/test/Bombus_bimaculatus data2/test/Bombus_bimaculatus\n",
    "    %cp -r data/test/Bombus_griseocollis data2/test/Bombus_griseocollis\n",
    "    %cp -r data/test/Bombus_impatiens data2/test/Bombus_impatiens\n",
    "    \n",
    "    # train\n",
    "    train(max_epochs = 160, min_epochs = 70, epoch_stretch = 15, train_path = ABS_PATH_TRAIN, valid_path = ABS_PATH_VALID, test_path = ABS_PATH_TEST, labels = classes, batch_size = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
